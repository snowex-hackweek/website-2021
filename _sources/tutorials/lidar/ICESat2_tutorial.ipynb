{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import hvplot.xarray\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# N.B.  This notebook is a lot more interesting if initialized with \n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-genre",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "ICESat-2 is a laser altimeter designed to precisely measure the height of snow and ice surfaces using green lasers with small footprints.  Although ICESat-2 doesn't measure surface heights with the same spatial density as airborne laser altimeters, its global spatial coverage makes it a tempting source of free data about snow surfaces.  In this tutorial we will:\n",
    "\n",
    "1. Give a brief overview of ICESat-2\n",
    "\n",
    "2. Show how to find ICESat-2 granues using the IcePyx metadata search tool\n",
    "\n",
    "3. Download some ATL03 photon data from the openAltimetry web service\n",
    "\n",
    "4. Request custom processed height estimates from the SlideRule project.\n",
    "\n",
    "## ICESat-2 measurements and coverage\n",
    "\n",
    "ICESat-2 measures surface heights with six laser beams, grouped into three pairs separated by 3 km, with a 90-m separation between the beams in each pair.\n",
    "\n",
    "Here's a sketch of how this looks (image credit: NSIDC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://nsidc.org/sites/nsidc.org/files/images/atlas-beam-pattern.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-november",
   "metadata": {},
   "source": [
    "ICESat-2 flies a repeat orbit with 1387 ground tracks every 91 days, but over Grand Mesa, the collection strategy (up until now) has designed to optimize spatial coverage, so the measurements are shifted to the left and right of the repeat tracks to help densify the dataset.  We should expect to see tracks running (approximately) north-south over the Mesa, in tripplets of pairs that are scattered from east to west.   Because clouds often block the laser, not every track will return usable data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://nsidc.org/sites/nsidc.org/files/images/icesat-2-spots-beams-fwd-rev.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-slovak",
   "metadata": {},
   "source": [
    "We describe ICESat-2's beam layout on the ground based on pairs (numbered 1, 2, and 3, from left to right) and the location of each beam in each pair (L, R).  Thus GT2L is the left beam in the center pair.  In each pair, one beam is always stronger than the other (to help penetrate thin clouds), but since the spacecraft sometimes reverses its orientation to keep the solar panels illuminated, the strong beam can be either left or right, depending on the phase of the mission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-density",
   "metadata": {},
   "source": [
    "## Basemap (Sentinel)\n",
    "\n",
    "To get a sense of where the data are, we're going to use an Sentinel SAR image of Grand Mesa.  I've stolen this snippet of code from the SAR tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDAL environment variables to efficiently read remote data\n",
    "os.environ['GDAL_DISABLE_READDIR_ON_OPEN']='EMPTY_DIR' \n",
    "os.environ['AWS_NO_SIGN_REQUEST']='YES' \n",
    "\n",
    "# SAR Data are stored in a public S3 Bucket\n",
    "url = 's3://sentinel-s1-rtc-indigo/tiles/RTC/1/IW/12/S/YJ/2016/S1B_20161121_12SYJ_ASC/Gamma0_VV.tif'\n",
    "\n",
    "# These Cloud-Optimized-Geotiff (COG) files have 'overviews', low-resolution copies for quick visualization\n",
    "XR=[725000.0, 767000.0]\n",
    "YR=[4.30e6, 4.34e6]\n",
    "# open the dataset\n",
    "da = rioxarray.open_rasterio(url, overview_level=1).squeeze('band')#.clip_box([712410.0, 4295090.0, 797010.0, 4344370.0])\n",
    "da=da.where((da.x>XR[0]) & (da.x < XR[1]), drop=True)\n",
    "da=da.where((da.y>YR[0]) & (da.y < YR[1]), drop=True)\n",
    "dx=da.x[1]-da.x[0]\n",
    "SAR_extent=[da.x[0]-dx/2, da.x[-1]+dx/2, np.min(da.y)-dx/2, np.max(da.y)+dx/2]\n",
    "\n",
    "# Prepare coordinate transformations into the basemap coordinate system\n",
    "from pyproj import Transformer, CRS\n",
    "crs=CRS.from_wkt(da['spatial_ref'].spatial_ref.crs_wkt)\n",
    "to_image_crs=Transformer.from_crs(crs.geodetic_crs, crs)\n",
    "to_geo_crs=Transformer.from_crs(crs, crs.geodetic_crs)\n",
    "\n",
    "corners_lon, corners_lat=to_geo_crs.transform(np.array(XR)[[0, 1, 1, 0, 0]], np.array(YR)[[0, 0, 1, 1, 0]])\n",
    "lonlims=[np.min(corners_lat), np.max(corners_lat)]\n",
    "latlims=[np.min(corners_lon), np.max(corners_lon)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-investor",
   "metadata": {},
   "source": [
    "## Searching for ICESat-2 data using IcePyx\n",
    "\n",
    "The IcePyx library has functions for searching for ICEsat-2 data, as well as subsetting it and retrieving it from NSIDC.  We're going to use the search functions today, because we don't need to retrieve the complete ICESat-2 products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import icepyx as ipx\n",
    "\n",
    "region_a = ipx.Query('ATL03', [lonlims[0], latlims[0], lonlims[1], latlims[1]], ['2018-12-01','2021-06-01'], \\\n",
    "                          start_time='00:00:00', end_time='23:59:59')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-subscriber",
   "metadata": {},
   "source": [
    "To run this next section, you'll need to setup your netrc file to connect to nasa earthdata.  During the hackweek we will use machine credentials, but afterwards, you may need to use your own credentials.  The login procedure is in the next cell, commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "#earthdata_uid = 'your_name_here'\n",
    "#email = 'your@email'\n",
    "#region_a.earthdata_login(earthdata_uid, email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-sculpture",
   "metadata": {},
   "source": [
    "Once we're logged in, the avail_granules() fetches a list of available ATL03 granules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.avail_granules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-clock",
   "metadata": {},
   "source": [
    "The filename for each granule (which contains lots of handy information) is in the 'producer_granule_id' field: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.granules.avail[0]['producer_granule_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-effect",
   "metadata": {},
   "source": [
    "The filename contains ATL03_YYYYMMDDHHMMSS_TTTTCCRR_rrr_vv.h5 where:\n",
    "\n",
    " * YYYMMDDHHMMSS gives the date (to the second) of the start of the granule\n",
    " * TTTT gives the ground-track number\n",
    " * CC gives the cycle number \n",
    " * RR gives the region (what part of the orbit this is) \n",
    " * rrr_vv give the release and version\n",
    " \n",
    " Let's strip out the date using a regular expression, and see when ICESat-2 flew over Grand Mesa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATLAS_re=re.compile('ATL.._(?P<year>\\d\\d\\d\\d)(?P<month>\\d\\d)(?P<day>\\d\\d)\\d+_(?P<track>\\d\\d\\d\\d)')\n",
    "\n",
    "date_track=[]\n",
    "for count, item in enumerate(region_a.granules.avail):\n",
    "    granule_info=ATLAS_re.search(item['producer_granule_id']).groupdict()\n",
    "    date_track += [ ('-'.join([granule_info[key] for key in ['year', 'month', 'day']]), granule_info['track'])]\n",
    "\n",
    "# print the first ten dates and ground tracks, plus their indexes\n",
    "[(count, dt) for count, dt in enumerate(date_track[0:10])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-device",
   "metadata": {},
   "source": [
    "From this point, the very capable icepyx interface allows you to order either full data granules or subsets of granules from NSIDC.  Further details are available from https://icepyx.readthedocs.io/en/latest/, and their 'examples' pages are quite helpful.  Note that ATL03 photon data granules are somewhat cumbersome, so downloading them without subsetting will be time consuming, and requesting subsetting from NSIDC may take a while.  \n",
    "\n",
    "## Ordering photon data from openAltimetry\n",
    "For ordering small numbers of points (up to one degree worth of data), the openAltimetry service provides very quick and efficient access to a simplified version of the ATL03 data.  Their API (https://openaltimetry.org/data/swagger-ui/) allows us to build web queries for the data.  We'll use that for a quick look at the data over Grand Mesa, initially reading just one central beam pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OA(date_track, lonlims, latlims, beamnames=[\"gt1l\",\"gt1r\",\"gt2l\",\"gt2r\",\"gt3l\",\"gt3r\"]):\n",
    "    '''\n",
    "    retrieve ICESat2 ATL03 data from openAltimetry\n",
    "    \n",
    "    Inputs:\n",
    "        date_track: a list of tuples.  Each contains a date string \"YYYY-MM-DD\" and track number (4-character string)\n",
    "        lonlims: longitude limits for the search\n",
    "        latlims: latitude limits for the search\n",
    "        beamnames: list of strings for the beams\n",
    "    outputs:\n",
    "        a dict containing ATL03 data by beam name\n",
    "    \n",
    "    Due credit:\n",
    "        Much of this code was borrowed Philipp Arndt's Pond Picker repo: https://github.com/fliphilipp/pondpicking\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    IS2_data={}\n",
    "    for this_dt in date_track:\n",
    "        this_IS2_data={}\n",
    "        for beamname in beamnames:\n",
    "            oa_url = 'https://openaltimetry.org/data/api/icesat2/atl03?minx={minx}&miny={miny}&maxx={maxx}&maxy={maxy}&trackId={trackid}&beamName={beamname}&outputFormat=json&date={date}&client=jupyter'\n",
    "            oa_url = oa_url.format(minx=lonlims[0],miny=latlims[0],maxx=lonlims[1], maxy=latlims[1], \n",
    "                                   trackid=this_dt[1], beamname=beamname, date=this_dt[0], sampling='true')\n",
    "            #.conf_ph = ['Noise','Buffer', 'Low', 'Medium', 'High']\n",
    "            if True:\n",
    "                r = requests.get(oa_url)\n",
    "                data = r.json()\n",
    "                D={}\n",
    "                D['lat_ph'] = []\n",
    "                D['lon_ph'] = []\n",
    "                D['h_ph'] = []\n",
    "                D['conf_ph']=[]\n",
    "                conf_ph = {'Noise':0, 'Buffer':1, 'Low':2, 'Medium':3, 'High':4}\n",
    "                for beam in data:\n",
    "                    for photons in beam['series']:\n",
    "                        for conf, conf_num in conf_ph.items():         \n",
    "                            if conf in photons['name']:\n",
    "                                for p in photons['data']:\n",
    "                                    \n",
    "                                    D['lat_ph'].append(p[0])\n",
    "                                    D['lon_ph'].append(p[1])\n",
    "                                    D['h_ph'].append(p[2])\n",
    "                                    D['conf_ph'].append(conf_num)\n",
    "                    D['x_ph'], D['y_ph']=to_image_crs.transform(D['lat_ph'], D['lon_ph'])\n",
    "                for key in D:\n",
    "                    D[key]=np.array(D[key])\n",
    "                if len(D['lat_ph']) > 0:\n",
    "                    this_IS2_data[beamname]=D\n",
    "            #except Exception as e:\n",
    "            #    print(e)\n",
    "            #    pass\n",
    "        if len(this_IS2_data.keys()) > 0:\n",
    "            IS2_data[this_dt] = this_IS2_data\n",
    "    return IS2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submitting all of these requests should take about 1 minute\n",
    "IS2_data=get_OA(date_track, lonlims, latlims, ['gt2l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(np.array(da)[::-1,:], origin='lower', extent=SAR_extent, cmap='gray', clim=[0, 0.5])#plt.figure();\n",
    "\n",
    "for dt, day_data in IS2_data.items():\n",
    "    for beam, D in day_data.items():\n",
    "        plt.plot(D['x_ph'][::10], D['y_ph'][::10], '.', markersize=3, label=str(dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-tourist",
   "metadata": {},
   "source": [
    "What we see in this plot is Grand Mesa, with lines showing data from the center beams of several tracks passing across it.  A few of these tracks have been repeated, but most are offset from the others.  Looking at these, it should be clear that the quality of the data is not consistent from track to track.  Some are nearly continuous, others have gaps, and other still have no data at all and are not plotted here.  Remember, though, that what we've plotted here are just the center beams.  There are a total of two more beam pairs, and a total of five more beams!\n",
    "\n",
    "To get an idea of what the data look like, we'll pick one of the tracks and plot its elevation profile.  In interactive mode (%matplotlib widget) it's possible to zoom in on the plot, query the x and y limits, and use these to identify the data for the track that intersects an area of interest.  I've done this to pick two good-looking tracks, but you can uncomment the first two lines here and zoom in yourself to look at other tracks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "XR=plt.gca().get_xlim()\n",
    "YR=plt.gca().get_ylim()\n",
    "print(XR)\n",
    "print(YR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XR=plt.gca().get_xlim()\n",
    "#YR=plt.gca().get_ylim()\n",
    "XR=(740773.7483556366, 741177.9430390946)\n",
    "YR=(4325197.508090873, 4325728.013612912)\n",
    "\n",
    "dts_in_axes=[]\n",
    "for dt, day_data in IS2_data.items():\n",
    "    for beam, D in day_data.items():\n",
    "        if np.any(\n",
    "            (D['x_ph'] > XR[0]) & (D['x_ph'] < XR[1]) &\n",
    "            (D['y_ph'] > np.min(YR)) & (D['y_ph'] < np.max(YR))):\n",
    "            dts_in_axes += [dt]\n",
    "dts_in_axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-mambo",
   "metadata": {},
   "source": [
    "Based on the axis limits I filled in, Track 295 has two repeats over the mesa that nearly coincide.\n",
    "\n",
    "Now we can get the full (six-beam) dataset for one of these repeats and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_track_data=get_OA([dts_in_axes[0]], lonlims, latlims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig=plt.figure(); \n",
    "hax=fig.subplots(1, 2)\n",
    "plt.sca(hax[0])\n",
    "plt.imshow(np.array(da)[::-1,:], origin='lower', extent=SAR_extent, cmap='gray', clim=[0, 0.5])#plt.figure();\n",
    "\n",
    "for dt, day_data in full_track_data.items():\n",
    "    for beam, D in day_data.items():\n",
    "        plt.plot(D['x_ph'], D['y_ph'],'.', markersize=1)\n",
    "plt.title(dts_in_axes[0])\n",
    "\n",
    "plt.sca(hax[1])\n",
    "D=day_data['gt2l']\n",
    "colors_key={((0,1)):'k', (2,3,4):'r'}\n",
    "for confs, color in colors_key.items():\n",
    "    for conf in confs:\n",
    "        these=np.flatnonzero(D['conf_ph']==conf)\n",
    "        plt.plot(D['y_ph'][these], D['h_ph'][these],'.', color=color, markersize=1)#label=','.join(list(confs)))\n",
    "plt.ylabel('WGS-84 height, m');\n",
    "plt.xlabel('UTM-12 northing, m');\n",
    "plt.title('gt2l');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-oxygen",
   "metadata": {},
   "source": [
    "On the left we see a plot of all six beams crossing (or almost crossing) Grand Mesa, in April of 2020.  If you zoom in on the plot, you can distinguish the beam pairs into separate beams.  On the right, we see one of the central beams crossing the mesa from south to north.  There is a broad band of noise photons that were close enough to the ground to be telemetered by the satellite, and a much narrower band (in red) of photons identified by the processing software as likely coming from the ground."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-navigation",
   "metadata": {},
   "source": [
    "These data give a maximum of detail about what the surface looks like to ICESat-2.  to reduce this to elevation data, telling the surface height at specific locations, there are a few options:\n",
    "    \n",
    "    1. Download higher-level products (i.e. ATL06, ATL08) from NSIDC\n",
    "    2. Calculate statistics of the photons (i.e. a running mean of the flagged photon heights\n",
    "    3. Ask the SlideRule service to calculate along-track averages of the photon heights.\n",
    "\n",
    "We're going to try (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-scott",
   "metadata": {},
   "source": [
    "## Ordering surface-height segments from SlideRule\n",
    "\n",
    "SildeRule is a new and exciting (to me) system that does real-time processing of ICESat-2 data _in the cloud_ while also offering efficient web-based delivery of data products.  It's new, and it's not available for all locations, but Grand Mesa is one of the test sites, so we should be able to get access to the full set of ATL03 data there.\n",
    "[MORE WORK TO GO HERE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-thumb",
   "metadata": {},
   "source": [
    "You'll need to install the sliderule-python package, available from https://github.com/ICESat2-SlideRule/sliderule-python\n",
    "This package has been installed on the hub, but if you need it, these commands will install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! [ -d sliderule-python ] || git clone https://github.com/ICESat2-SlideRule/sliderule-python.git \n",
    "#! cd sliderule-python; python setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-lender",
   "metadata": {},
   "source": [
    "We will submit a query to sliderule to process all of the data that CMR finds for our region, fitting 20-meter line-segments to all of the photons with medium-or-better signal confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sliderule import icesat2\n",
    "\n",
    "# initialize\n",
    "icesat2.init(\"icesat2sliderule.org\", verbose=False)\n",
    "\n",
    "# region of interest polygon\n",
    "region = [ {\"lon\":lon_i, \"lat\":lat_i} for lon_i, lat_i in \n",
    "          zip(np.array(lonlims)[[0, -1, -1, 0, 0]],  np.array(latlims)[[0, 0, -1, -1, 0]])]\n",
    "\n",
    "# request parameters\n",
    "params = {\n",
    "    \"poly\": region,  # request the polygon defined by our lat-lon bounds\n",
    "    \"srt\": icesat2.SRT_LAND, # request classification based on the land algorithm\n",
    "    \"cnf\": icesat2.CNF_SURFACE_MEDIUM, # use all photons of low confidence or better\n",
    "    \"len\": 20.0,  # fit data in overlapping 40-meter segments\n",
    "    \"res\": 10.0,  # report one height every 20 m\n",
    "    \"ats\":5., #report a segment only if it contains at least 2 photons separated by 5 m\n",
    "    \"maxi\": 6,  # allow up to six iterations in fitting each segment to the data\n",
    "}\n",
    "\n",
    "# make request\n",
    "rsps = icesat2.atl06p(params, \"atlas-s3\")\n",
    "\n",
    "# save the result in a dataframe\n",
    "df = pd.DataFrame(rsps)\n",
    "\n",
    "# calculate the polar-stereographic coordinates:\n",
    "df['x'], df['y']=to_image_crs.transform(df['lat'], df['lon'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-warrant",
   "metadata": {},
   "source": [
    "SlideRule complains when it tries to calculate heights within our ROI for ground tracks that don't intersect the ROI.  This happens quite a bit because the CMR service that IcePyx and SlideRule use to search for the data uses a generous buffer on each ICESat-2 track.  It shouldn't bother us.  In fact, we have quite a few tracks for our region.\n",
    "\n",
    "Let's find all the segments from rgt 295, cycle 7 and map their heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "plt.imshow(np.array(da)[::-1,:], origin='lower', extent=SAR_extent, cmap='gray', clim=[0, 0.5])#plt.figure();\n",
    "ii=(df['rgt']==295) & (df['cycle']==7)\n",
    "plt.scatter(df['x'][ii], df['y'][ii],4, c=df['h_mean'][ii], cmap='gist_earth')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-press",
   "metadata": {},
   "source": [
    "As we saw a few cells up, for track 295 cycles 7 and 8 are nearly exact repeats.  Cycle 7 was April 2020, cycle 8 was July 2020.  Could it be that we can measure snow depth in April by comparing the two?  Let's plot spot 3 for both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure();\n",
    "ii=(df['rgt']==295) & (df['cycle']==7) & (df['spot']==3)\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii],'.', label='April')\n",
    "ii=(df['rgt']==295) & (df['cycle']==8) & (df['spot']==3)\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii],'.', label='July')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('polar stereographic northing, m')\n",
    "plt.ylabel('height, m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-corner",
   "metadata": {},
   "source": [
    "To try to get at snow depth, we can look for bare-earth DTMs here:\n",
    "    'https://prd-tnm.s3.amazonaws.com/LidarExplorer/index.html#'\n",
    "I've picked one of the 1-meter DTMs that covers part of track 295.  We'll read it directly from s3 with the rasterio/xarray package, and downsample it to 3m (to save time later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "from rasterio.enums import Resampling\n",
    "\n",
    "url='https://prd-tnm.s3.amazonaws.com/StagedProducts/Elevation/1m/Projects/CO_MesaCo_QL2_UTM12_2016/TIFF/USGS_one_meter_x74y433_CO_MesaCo_QL2_UTM12_2016.tif'\n",
    "\n",
    "lidar_ds=rxr.open_rasterio(url)\n",
    "#resample the DTM to ~3m:\n",
    "scale_factor = 1/3\n",
    "new_width = int(lidar_ds.rio.width * scale_factor)\n",
    "new_height = int(lidar_ds.rio.height * scale_factor)\n",
    "\n",
    "#reproject the horizontal CRS to match ICESat-2\n",
    "UTM_wgs84_crs=CRS.from_epsg(32612)\n",
    "lidar_3m = lidar_ds.rio.reproject(\n",
    "    UTM_wgs84_crs,\n",
    "    shape=(new_height, new_width),\n",
    "    resampling=Resampling.bilinear,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "lidar_3m.sel(band=1).plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-titanium",
   "metadata": {},
   "source": [
    "To compare the DTM directly with the ICESat-2 data, we'll need to sample it at the ICESat-2 points.  There are probably ways to do this directly in xarray, but I'm not an expert.  Here we'll use a scipy interpolator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import RectBivariateSpline\n",
    "interpolator = RectBivariateSpline(np.array(lidar_3m.y)[::-1], np.array(lidar_3m.x), \n",
    "                                   np.array(lidar_3m.sel(band=1))[::-1,:], kx=1, ky=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array(lidar_3m.x)\n",
    "y0=np.array(lidar_3m.y)\n",
    "\n",
    "ii=(df['rgt']==295) & (df['cycle']==7) & (df['spot']==3)\n",
    "ii &= (df['x'] > np.min(x0)) & (df['x'] < np.max(x0))\n",
    "ii &= (df['y'] > np.min(y0)) & (df['y'] < np.max(y0))\n",
    "\n",
    "zi=interpolator.ev(df['y'][ii], df['x'][ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=[8, 5]); \n",
    "hax=fig.subplots(1,2)\n",
    "plt.sca(hax[0])\n",
    "lidar_3m.sel(band=1).plot.imshow()\n",
    "plt.plot(df['x'][ii], df['y'][ii],'.')\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.sca(hax[1])\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii],'.', label='April')\n",
    "plt.plot(df['y'][ii], zi,'.', label='DTM')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-nurse",
   "metadata": {},
   "source": [
    "The DTM is below the April ICESat-2 heights.  That's probably not right, and it's because we don't have the vertical datums correct here (ICESat-2 WGS84, the DEM is NAD83).  That's OK!  Since we have multiple passes over the same DEM, we can use the DEM to correct for spatial offsets between the measurements.  Let's use the DEM to correct for differences between the July and April data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "ii=(df['rgt']==295) & (df['cycle']==7) & (df['spot']==3)\n",
    "ii &= (df['x'] > np.min(x0)) & (df['x'] < np.max(x0))\n",
    "ii &= (df['y'] > np.min(y0)) & (df['y'] < np.max(y0))\n",
    "zi=interpolator.ev(df['y'][ii], df['x'][ii])\n",
    "\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii]-zi,'.', label='April')\n",
    "\n",
    "ii=(df['rgt']==295) & (df['cycle']==8) & (df['spot']==3)\n",
    "ii &= (df['x'] > np.min(x0)) & (df['x'] < np.max(x0))\n",
    "ii &= (df['y'] > np.min(y0)) & (df['y'] < np.max(y0))\n",
    "zi=interpolator.ev(df['y'][ii], df['x'][ii])\n",
    "\n",
    "\n",
    "plt.plot(df['y'][ii], df['h_mean'][ii]-zi,'.', label='July')\n",
    "plt.gca().set_ylim([-20, -10])\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-minister",
   "metadata": {},
   "source": [
    "This looks good, if a little noisy.  We could get a better comparison by (1) using multiple ICESat-2 tracks to extract a mean snow-off difference between the DTM and ICESat-2, or (2). finding adjacent pairs of measurements  between the two tracks, and comparing their heights directly.  These are both good goals for projects!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-madness",
   "metadata": {},
   "source": [
    "## Further reading:\n",
    "\n",
    "\n",
    "There are lots of resources available for ICESat-2 data on the web.  Two of the best are the NSIDC ICESat-2 pages:\n",
    "\n",
    "https://nsidc.org/data/icesat-2\n",
    "\n",
    "and NASA's ICESat-2 page:\n",
    "https://icesat-2.gsfc.nasa.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-simon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
