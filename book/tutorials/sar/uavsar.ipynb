{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "altered-grade",
   "metadata": {},
   "source": [
    "# UAVSAR\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "*A 30 minute guide to UAVSAR data for SnowEX*\n",
    "- overview of UAVSAR data (both InSAR and PolSAR products)\n",
    "- demonstrate how to access and transform data\n",
    "- use Python rasterio and matplotlib to display the data\n",
    "```\n",
    "\n",
    ":::{figure-md} UAVSAR\n",
    "<img src=\"../../img/UAVSAR_plane.jpg\" alt=\"uavsar airplane\" width=\"800px\">\n",
    "\n",
    "Picture of UAVSAR. [Source](https://asf.alaska.edu/data-sets/sar-data-sets/uavsar/)\n",
    ":::\n",
    "\n",
    "\n",
    "Intro slide deck: https://uavsar.jpl.nasa.gov/education/what-is-uavsar.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import re\n",
    "import zipfile\n",
    "import getpass\n",
    "from osgeo import gdal \n",
    "import os  # for chdir, getcwd, path.basename, path.exists\n",
    "import hvplot.xarray\n",
    "import pandas as pd # for DatetimeIndex \n",
    "import rioxarray\n",
    "import numpy as np #for log10, mean, percentile, power\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show # plotting raster data\n",
    "from rasterio.plot import show_hist #histograms of raster data\n",
    "import codecs # for text parsing code\n",
    "import glob # for listing files in tiff conversion function\n",
    "import matplotlib.pyplot as plt # for add_subplot, axis, figure, imshow, legend, plot, set_axis_off, set_data,\n",
    "                                # set_title, set_xlabel, set_ylabel, set_ylim, subplots, title, twinx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-sperm",
   "metadata": {},
   "source": [
    "## What is UAVSAR?\n",
    "\n",
    "UAVSAR stands for uninhabited aerial vehicle synthetic aperture radar. It is a suborbital (airplane) remote sensing instrument operated out of NASA JPL.\n",
    "\n",
    "| frequency (cm) | resolution (rng x azi m) | swath width (km) |\n",
    "| - | - | - | \n",
    "| L-band 23| 1.8 x 5.5 | 16 | \n",
    "\n",
    "Documentation:\n",
    "* https://uavsar.jpl.nasa.gov/education/what-is-uavsar.html\n",
    "* https://asf.alaska.edu/data-sets/sar-data-sets/uavsar/\n",
    "* https://ieeexplore-ieee-org.unr.idm.oclc.org/document/1631770 (Rosen et al. 2006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-confidentiality",
   "metadata": {},
   "source": [
    "## NASA SnowEx 2020 and 2021 UAVSAR Campaings\n",
    "\n",
    "During the winter of 2020 and 2021, NASA conducted an L-band InSAR timeseris at a seris of sites across the Western US with the goal of tracking changes in SWE. Get site coordinate from HP to make map!!!!\n",
    "\n",
    ":::{figure-md} UAVSAR-map\n",
    "<img src=\"../../img/SnowEx2020.png\" alt=\"uavsar map\" width=\"800px\">\n",
    "\n",
    "Map of the UAVSAR flight locations for NASA SnowEx. Source: Chris Hiemstra\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-howard",
   "metadata": {},
   "source": [
    "## Data Access\n",
    "\n",
    "There are multiple ways to access UAVSAR data. Also the SQL database.\n",
    "\n",
    "* [Alaska Satellite Facility Vertex Portal](https://search.asf.alaska.edu/#/?dataset=UAVSAR)\n",
    "* [NASA Earthdata Suborbital Search](https://search.earthdata.nasa.gov/portal/suborbital/search?fi=UAVSAR&as[instrument][0]=UAVSAR)\n",
    "* [JPL UAVSAR Data Search](https://uavsar.jpl.nasa.gov/cgi-bin/data.pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-picture",
   "metadata": {},
   "source": [
    "```{admonition} InSAR Data Types\n",
    ":class: InSAR Data Types\n",
    "- ANN file (.ann): a text annotation file with metadata\n",
    "- AMP files (.amp1 and .amp2): calibrated multi-looked amplitude products\n",
    "- INT files (.int): interferogram product, complex number format (we won't be using these here)\n",
    "- COR files (.cor): interferometric correlation product, a measure of the noise level of the phase\n",
    "- GRD files (.grd): interferometric products projected to the ground in simple geographic coordinates (latitude, longitude)\n",
    "- HGT file  (.hgt): the DEM that was used in the InSAR processing\n",
    "- KML and KMZ files (.kml or .kmz): format for viewing files in Google Earth (can't be used for analysis)\n",
    "```\n",
    "\n",
    "```{admonition} PolSAR Data Types\n",
    ":class: PolSAR Data Types\n",
    "-\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-poker",
   "metadata": {},
   "source": [
    "### Data Download\n",
    "\n",
    "We will use our NASA EarthData credentials and ASF Vertex to download an InSAR pair data into our notebook directly. For this tutorial, we will be working with UAVSAR data from February of 2020. If you want to use different data in the future, change the links in the files variable. The screengrab below shows how I generated these download links from the ASF site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-webster",
   "metadata": {},
   "source": [
    ":::{figure-md} vertex\n",
    "<img src=\"../../img/asf_vertex.png\" alt=\"asf vertex\" width=\"800px\">\n",
    "\n",
    "Screenshot of ASF Vertex interface\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NASA EARTHDATA Credentials from ~/.netrc or manual input\n",
    "import netrc\n",
    "\n",
    "try:\n",
    "    os.chmod('/home/jovyan/.netrc', 0o600) # only needed if running on jupyterhub\n",
    "    (ASF_USER, account, ASF_PASS) = netrc.netrc().authenticators(\"urs.earthdata.nasa.gov\")\n",
    "except:\n",
    "    ASF_USER = input(\"Enter Username: \")\n",
    "    ASF_PASS = getpass.getpass(\"Enter Password: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory in which the notebook resides\n",
    "if 'tutorial_home_dir' not in globals():\n",
    "     tutorial_home_dir = os.getcwd()\n",
    "print(\"Notebook directory: \", tutorial_home_dir)\n",
    "\n",
    "if not os.path.exists('/tmp/'):\n",
    "    os.chdir('/tmp')\n",
    "\n",
    "data_dir = os.path.join('/tmp')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "files = ['https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/grmesa_27416_20003-028_20005-007_0011d_s01_L090_01_int_grd.zip',\n",
    "        'https://datapool.asf.alaska.edu/AMPLITUDE_GRD/UA/grmesa_27416_20003-028_20005-007_0011d_s01_L090_01_amp_grd.zip']\n",
    "    \n",
    "for file in files:\n",
    "    print(f'downloading {file}...')\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(data_dir,filename)):\n",
    "        cmd = \"wget {0} --user={1} --password={2} -P {3} -nc\".format(file, ASF_USER, ASF_PASS, data_dir)\n",
    "        os.system(cmd)\n",
    "    else:\n",
    "        print(filename + \" already exists. Skipping download ..\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if downloaded\n",
    "# the *.* syntax means print all files in the directory\n",
    "\n",
    "print(glob.glob(\"/tmp/*.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for deleting contents of temp directory if needed. DO NOT uncomment, this will cause the notebook to fail.\n",
    "\n",
    "#files = glob.glob(\"/tmp/*.*\")\n",
    "#for f in files:\n",
    "#    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-compound",
   "metadata": {},
   "source": [
    "### Unzipping the files we just downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unzip files just downloaded\n",
    "\n",
    "# define file path for both files\n",
    "int_zip = '/tmp/grmesa_27416_20003-028_20005-007_0011d_s01_L090_01_int_grd.zip'\n",
    "amp_zip = '/tmp/grmesa_27416_20003-028_20005-007_0011d_s01_L090_01_amp_grd.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip\n",
    "\n",
    "# int\n",
    "with zipfile.ZipFile(int_zip, \"r\") as zip_ref:\n",
    "    zip_ref.printdir()\n",
    "    print('Extracting all the files now...')\n",
    "    zip_ref.extractall('/tmp')\n",
    "    print(\"done\")\n",
    "    \n",
    "# amp\n",
    "with zipfile.ZipFile(amp_zip, \"r\") as zip_ref:\n",
    "    zip_ref.printdir()\n",
    "    print('Extracting all the files now...')\n",
    "    zip_ref.extractall('/tmp')\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-investor",
   "metadata": {},
   "source": [
    "### Removing unwanted data\n",
    "For simplicity, we'll only work with HH polarization. The three other polarizations (VV, VH, HV) provide additional information about the surface properties and can be utilized in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up unwanted data from what we just downloaded\n",
    "\n",
    "directory = '/tmp'\n",
    "os.chdir(directory)\n",
    "HV_files = glob.glob('*HV_01*') #define all HV\n",
    "VV_files = glob.glob('*VV_01*') #define all VV\n",
    "VH_files = glob.glob('*VH_01*') #define all VH\n",
    "zips = glob.glob('*.zip') # define the zip files\n",
    "\n",
    "# loops to remove them\n",
    "\n",
    "for f in HV_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in VV_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in VH_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in zips:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see what files are left in the directory\n",
    "\n",
    "print(glob.glob(\"/tmp/*.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-exhibit",
   "metadata": {},
   "source": [
    "##\n",
    "Now we only have the HH polarization, the annoation file, and the 6 .grd files!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-motorcycle",
   "metadata": {},
   "source": [
    "## Converting Data to GeoTiffs\n",
    "\n",
    "The downloadable UAVSAR data comes in a flat binary format (.grd), which is not readable by GDAL (Geospatial Data Abstraction Library). Therefore it needs to be transformed for use in standard spatial analysis software (ArcGIS, QGIS, Python, R, MATLAB, etc.). To do this, we will use the uavsar_tiff_convert function, which takes information (latitude, longitude, number of lines and samples, data type, pixel size) from the annotation file to create an ENVI header (.hdr). Once the ENVI header is created, the files can be read into Python and converted to GeoTiffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's print the annotation file to get a look at it's content\n",
    "# these file contain a lot of information and can be very intimidating and hard to understand, but being able to read them is vital to working this UAVSAR data\n",
    "\n",
    "#with open('/tmp/grmesa_27416_20003-028_20005-007_0011d_s01_L090HH_01.ann') as f:\n",
    "#    print(f.read())\n",
    "!head -n 15 /tmp/grmesa_27416_20003-028_20005-007_0011d_s01_L090HH_01.ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-california",
   "metadata": {},
   "source": [
    "##\n",
    "This function pulls out information from the annotation file, builds and ENVI header, and then converts the data to GeoTIFFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder is path to a folder with an .ann (or .txt) and .grd files (.amp1, .amp2, .cor, .unw, .int)\n",
    "\n",
    "def uavsar_tiff_convert(folder):\n",
    "    \"\"\"\n",
    "    Builds a header file for the input UAVSAR .grd file,\n",
    "    allowing the data to be read as a raster dataset.\n",
    "    :param folder:   the folder containing the UAVSAR .grd and .ann files\n",
    "    \"\"\"\n",
    "\n",
    "    os.chdir(folder)\n",
    "    int_file = glob.glob(os.path.join(folder, 'int.grd'))\n",
    "\n",
    "    # Empty lists to put information that will be recalled later.\n",
    "    Lines_list = []\n",
    "    Samples_list = []\n",
    "    Latitude_list = []\n",
    "    Longitude_list = []\n",
    "    Files_list = []\n",
    "\n",
    "    # Step 1: Look through folder and determine how many different flights there are\n",
    "    # by looking at the HDR files.\n",
    "    for files in os.listdir(folder):\n",
    "        if files [-4:] == \".grd\":\n",
    "            newfile = open(files[0:-4] + \".hdr\", 'w')\n",
    "            newfile.write(\"\"\"ENVI\n",
    "description = {DESCFIELD}\n",
    "samples = NSAMP\n",
    "lines = NLINE\n",
    "bands = 1\n",
    "header offset = 0\n",
    "data type = DATTYPE\n",
    "interleave = bsq\n",
    "sensor type = UAVSAR L-Band\n",
    "byte order = 0\n",
    "map info = {Geographic Lat/Lon, \n",
    "            1.000, \n",
    "            1.000, \n",
    "            LON, \n",
    "            LAT,  \n",
    "            0.0000555600000000, \n",
    "            0.0000555600000000, \n",
    "            WGS-84, units=Degrees}\n",
    "wavelength units = Unknown\n",
    "                \"\"\"\n",
    "                          )\n",
    "            newfile.close()\n",
    "            if files[0:18] not in Files_list:\n",
    "                Files_list.append(files[0:18])\n",
    "\n",
    "    #Variables used to recall indexed values.\n",
    "    var1 = 0\n",
    "\n",
    "    #Step 2: Look through the folder and locate the annotation file(s).\n",
    "    # These can be in either .txt or .ann file types.\n",
    "    for files in os.listdir(folder):\n",
    "        if Files_list[var1] and files[-4:] == \".txt\" or files[-4:] == \".ann\":\n",
    "            #Step 3: Once located, find the info we are interested in and append it to\n",
    "            # the appropriate list. We limit the variables to <=1 so that they only\n",
    "            # return two values (one for each polarization of\n",
    "            searchfile = codecs.open(files, encoding = 'windows-1252', errors='ignore')\n",
    "            for line in searchfile:\n",
    "                if \"Ground Range Data Latitude Lines\" in line:\n",
    "                    Lines = line[65:70]\n",
    "                    print(f\"Number of Lines: {Lines}\")\n",
    "                    if Lines not in Lines_list:\n",
    "                        Lines_list.append(Lines)\n",
    "\n",
    "                elif \"Ground Range Data Longitude Samples\" in line:\n",
    "                    Samples = line[65:70]\n",
    "                    print(f\"Number of Samples: {Samples}\")\n",
    "                    if Samples not in Samples_list:\n",
    "                        Samples_list.append(Samples)\n",
    "\n",
    "                elif \"Ground Range Data Starting Latitude\" in line:\n",
    "                    Latitude = line[65:85]\n",
    "                    print(f\"Top left lat: {Latitude}\")\n",
    "                    if Latitude not in Latitude_list:\n",
    "                        Latitude_list.append(Latitude)\n",
    "\n",
    "                elif \"Ground Range Data Starting Longitude\" in line:\n",
    "                    Longitude = line[65:85]\n",
    "                    print(f\"Top left Lon: {Longitude}\")\n",
    "                    if Longitude not in Longitude_list:\n",
    "                        Longitude_list.append(Longitude)\n",
    "    \n",
    "                        \n",
    "                 \n",
    "            #Reset the variables to zero for each different flight date.\n",
    "            var1 = 0\n",
    "            searchfile.close()\n",
    "\n",
    "\n",
    "    # Step 3: Open .hdr file and replace data for all type 4 (real numbers) data\n",
    "    # this all the .grd files expect for .int\n",
    "    for files in os.listdir(folder):\n",
    "        if files[-4:] == \".hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = DATTYPE\" in line:\n",
    "                        sources.write(re.sub(line[12:19], \"4\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "    \n",
    "    # Step 3: Open .hdr file and replace data for .int file date type 6 (complex)                 \n",
    "    for files in os.listdir(folder):\n",
    "        if files[-8:] == \".int.hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = 4\" in line:\n",
    "                        sources.write(re.sub(line[12:13], \"6\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "                        \n",
    "    \n",
    "    # Step 4: Now we have an .hdr file, the data is geocoded and can be loaded into python with rasterio\n",
    "    # once loaded in we use gdal.Translate to convert and save as a .tiff\n",
    "    \n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.grd')) # list all .grd files\n",
    "    for data_path in data_to_process: # loop to open and translate .grd to .tiff, and save .tiffs using gdal\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tiff'), raster_dataset, format = 'Gtiff', outputType = gdal.GDT_Float32)\n",
    "    \n",
    "    # Step 5: Save the .int raster, needs seperate save because of the complex format\n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.int.grd')) # list all .int.grd files (only 1)\n",
    "    for data_path in data_to_process:\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tiff'), raster_dataset, format = 'Gtiff', outputType = gdal.GDT_CFloat32)\n",
    "\n",
    "    print(\".tiffs have been created\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/tmp' # define folder where the .grd and .ann files are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "uavsar_tiff_convert(data_folder) # call the tiff convert function, and it will print the information it extracted from the .ann file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-approval",
   "metadata": {},
   "source": [
    "##\n",
    "Now we'll delete the unneeded .grd and .hdr files that our tiffs have been created. If you're using this code on your local machine, this probably isn't absolutely necessary. The JupyterHub cloud we're working in has limited space, so deletion is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_folder)\n",
    "grd = glob.glob('*.grd') #define .grd\n",
    "hdr = glob.glob('*.hdr*') #define .hdr\n",
    "\n",
    "# remove both\n",
    "for f in grd:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in hdr:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what's in the directory, only .tiffs and our annotation file!\n",
    "print(glob.glob(\"*.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "### inspect our newly creaed .tiffs, and create named objects for each data type. We'll use these new obects in the next step\n",
    "\n",
    "# amplitude from the first aquisition\n",
    "for amp1 in glob.glob(\"*amp1.grd.tiff\"):\n",
    "    print(amp1)\n",
    "    \n",
    "# amplitude from the second aquisition\n",
    "for amp2 in glob.glob(\"*amp2.grd.tiff\"):\n",
    "    print(amp2)\n",
    "\n",
    "# coherence\n",
    "for cor in glob.glob(\"*cor.grd.tiff\"):\n",
    "    print(cor)\n",
    "\n",
    "# unwrapped phase\n",
    "for unw in glob.glob(\"*unw.grd.tiff\"):\n",
    "    print(unw)\n",
    "\n",
    "# dem used in processing\n",
    "for dem in glob.glob(\"*hgt.grd.tiff\"):\n",
    "    print(dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-marsh",
   "metadata": {},
   "source": [
    "Inspect the meta data the rasters using the rio (shorthand for rasterio) ```profile``` funciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "unw_rast = rio.open(unw)\n",
    "meta_data = unw_rast.profile\n",
    "print(meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-characteristic",
   "metadata": {},
   "source": [
    "## Opening and plotting the raw UAVSAR raster files\n",
    "We now have our five different data sets: the two amplitude files, coherence, unwrapped phased, and the DEM. We will not be working the actual interferogram (.int) file because it contains complex numbers that don't work in the Python packages being used.\n",
    "\n",
    "Here we will open a raster files using the ```rio.open()``` funciton. We'll then create a simple plot using the ```rio``` ```show()``` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-dover",
   "metadata": {},
   "source": [
    "### Amp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp1_rast = rio.open(amp1) #open raster\n",
    "fig, ax = plt.subplots(figsize = (10,7)) #define figure size\n",
    "ax.set_title(\"Amplitude 1\",fontsize = 16); #set title and font size\n",
    "show((amp1_rast, 1), cmap = 'Blues', vmin = 0, vmax = 1); #plot, set color type and range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-child",
   "metadata": {},
   "source": [
    "### Amp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp2_rast = rio.open(amp2)\n",
    "fig, ax = plt.subplots(figsize = (10,7))\n",
    "ax.set_title(\"Amplitude 2\",fontsize = 16);\n",
    "show((amp2_rast, 1), cmap = 'Reds', vmin = 0, vmax = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-religious",
   "metadata": {},
   "source": [
    "### Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_rast = rio.open(cor)\n",
    "fig, ax = plt.subplots(figsize = (10,7))\n",
    "ax.set_title(\"Coherence\",fontsize = 16);\n",
    "show((cor_rast, 1), cmap = 'inferno', vmin = 0, vmax = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-fifth",
   "metadata": {},
   "source": [
    "### DEM\n",
    "Now we'll make a quick histogram using ```rio``` ```show_hist()``` to check what we should set the bounds of the color scale to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_rast = rio.open(dem)\n",
    "show_hist(dem_rast, bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,7))\n",
    "ax.set_title(\"DEM\",fontsize = 16);\n",
    "show((dem_rast, 1), cmap = 'gist_earth', vmin = 1900, vmax = 3600); #estimated these values from the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-sessions",
   "metadata": {},
   "source": [
    "### Unwrapped Phase\n",
    "Another histogram for the color bounds, and note the large amount of 0's values. This is will become imporant in the next few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "unw_rast = rio.open(unw)\n",
    "show_hist(unw_rast, bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "unw_rast = rio.open(unw)\n",
    "fig, ax = plt.subplots(figsize = (10,7))\n",
    "ax.set_title(\"Unwrapped Phase\",fontsize = 16);\n",
    "show((unw_rast, 1), cmap = 'viridis', vmin = -3, vmax = 1.5); # info from histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-clarity",
   "metadata": {},
   "source": [
    "## Formatting the data for visualization\n",
    "The plots of the raw data need some work. Some fotmatting is necessay to visualize the data clearly. UAVSAR uses \"0\" as it's no data value (not the best practice in general) for amplitude, coherence, and unwrapped phase. For the DEM -10000 is the no data value. Using -9999 or another value that is obviously not actual data is a better practice with spatial data to limit confusion. We'll convert these no data values to NaN (Not a Number) which will remove the boarders around data, and in data lost the unwrapping in the UNW file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-greece",
   "metadata": {},
   "source": [
    "### Amplitude formatting\n",
    "For the two amplitude files we need to do two things. Convert from the linear amplitude scale to decibel (dB) and change the 0 values to NaN. To do this we'll convert our raster file to an ```np.array``` to manipulate it. Note that when we convert the raster data to an array, the spatial coordinates are lost and it no longer plots the x and y scales at longitude and latitude values. For our purposes this okay, but you would need to convert back to a .tiff if you wanted to save the file to use in ArcGIS or QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amp1 \n",
    "# open raster as a data array\n",
    "with rio.open(amp1) as amp1_raw:\n",
    "    amp1_array = amp1_raw.read(1) #open raster as an array\n",
    "\n",
    "# convert all 0's to nan\n",
    "amp1_array[amp1_array == 0] = np.nan # conver all 0's to NaN's\n",
    "\n",
    "# covert to dB\n",
    "amp1_dB = 10.0 * np.log10(amp1_array) # convert to the dB scale\n",
    "\n",
    "# amp2 \n",
    "with rio.open(amp2) as amp2_raw:\n",
    "    amp2_array = amp2_raw.read(1)\n",
    "    \n",
    "amp2_array[amp2_array == 0] = np.nan\n",
    "\n",
    "amp2_dB = 10.0 * np.log10(amp2_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist(amp2_dB, bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-complexity",
   "metadata": {},
   "source": [
    "##\n",
    "Instead of using the ```rio.show()``` function, we'll try out the ```matplotlib``` (we are calling ```plt```) ```im.show()``` style of plotting to impliment a color scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "ax.set_title(\"Amplitude 1 #data info here\", fontsize= 20) #title and font size\n",
    "amp2_plot = ax.imshow(amp2_dB, cmap='inferno',vmin=-16, vmax=0) #set bounds and color map\n",
    "\n",
    "# add legend\n",
    "colorbar = fig.colorbar(amp2_plot, ax=ax) #add color bar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-experience",
   "metadata": {},
   "source": [
    "#\n",
    "Now we'll create a function called ```show_two_images()``` to plot two images at once. The function inputs are a data array, color map name, and a plot title for both images. It uses ```np.nanpercentile()``` to automatically set the color scale bounds, but you can also set them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for showing two images using matplotlib\n",
    "plt.rcParams.update({'font.size': 12}) # set fontsize\n",
    "def show_two_images(img1, img2, col1, col2, title1, title2, vmin1=None, vmax1=None, vmin2=None, vmax2=None):\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    # auto setting axis limits\n",
    "    if vmin1 == None:\n",
    "        vmin1 = np.nanpercentile(img1, 1)\n",
    "    if vmax1 == None:\n",
    "        vmax1 = np.nanpercentile(img1, 99)\n",
    "    \n",
    "    # plot image\n",
    "    masked_array1 = np.ma.array(img1, mask=np.isnan(0)) #mask for 0\n",
    "    plt1 = ax1.imshow(masked_array1, cmap=col1, vmin=vmin1, vmax=vmax1, interpolation = 'nearest') #fixes NaN problem\n",
    "    ax1.set_title(title1)\n",
    "    ax1.xaxis.set_label_text('Linear stretch Min={} Max={}'.format(vmin1, vmax1))\n",
    "        \n",
    "    # add color scale\n",
    "    colorbar = fig.colorbar(plt1, ax=ax1, fraction=0.03, pad=0.04)\n",
    "    \n",
    "     # auto setting axis limits\n",
    "    if vmin2 == None:\n",
    "        vmin2 = np.nanpercentile(img2, 1)\n",
    "    if vmax2 == None:\n",
    "        vmax2 = np.nanpercentile(img2, 99)\n",
    "    \n",
    "    # plot image\n",
    "    masked_array2 = np.ma.array(img2, mask=np.isnan(0)) #mask for 0\n",
    "    plt2 = ax2.imshow(masked_array2, cmap=col2, vmin=vmin2, vmax=vmax2, interpolation = 'nearest')\n",
    "    ax2.set_title(title2)\n",
    "    ax2.xaxis.set_label_text('Linear stretch Min={} Max={}'.format(vmin2, vmax2))\n",
    "    colorbar = fig.colorbar(plt2, ax=ax2, fraction=0.03, pad=0.04)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both amplitude images\n",
    "\n",
    "show_two_images(amp1_dB, amp2_dB, 'gray', 'gray', 'Amp1_dB', 'Amp2_dB', -12,-1,-12,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-tactics",
   "metadata": {},
   "source": [
    "### Coherence, Unwrapped Phase, DEM\n",
    "For these three data types, we only need to convert no data values (0) to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.open(cor) as cor_raw:\n",
    "    cor_array = cor_raw.read(1)\n",
    "\n",
    "cor_array[cor_array == 0] = np.nan # convert all 0's to nan\n",
    "\n",
    "# unw\n",
    "with rio.open(unw) as unw_raw:\n",
    "    unw_array = unw_raw.read(1)\n",
    "    \n",
    "unw_array[unw_array == 0] = np.nan\n",
    "\n",
    "# dem\n",
    "with rio.open(dem) as dem_raw:\n",
    "    dem_array = dem_raw.read(1)\n",
    "    \n",
    "dem_array[dem_array == -10000] = np.nan #different no data value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-berkeley",
   "metadata": {},
   "source": [
    "#\n",
    "Checking to see if it worked by comparing ```unw_rast``` which still includes 0's to ```unw_array``` where we changed them to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hist(unw_rast, bins = 100) \n",
    "show_hist(unw_array, bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12}) # set fontsize\n",
    "show_two_images(dem_array, unw_array, 'gist_earth', 'viridis', 'DEM (m)', 'UNW (radians)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-fundamental",
   "metadata": {},
   "source": [
    "#\n",
    "Let's plot the UNW raster larger so we can get a better look at the detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16}) # increase plot font size for larger plot\n",
    "fig, ax = plt.subplots(figsize=(40, 20))\n",
    "\n",
    "masked_array = np.ma.array(unw_array, mask = np.isnan(0)) # mask for 0\n",
    "ax.set_title(\"UNW (radians)\", fontsize= 20) #title and font size\n",
    "img = ax.imshow(masked_array, cmap = 'viridis', interpolation = 'nearest', vmin = -2.5, vmax =1.5)\n",
    "\n",
    "# add legend\n",
    "colorbar = fig.colorbar(img, ax=ax, fraction=0.03, pad=0.04) # add color bar\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': 12}) # change font back to normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-saturn",
   "metadata": {},
   "source": [
    "## \n",
    "This looks **much** better! Plotting the image at a larger scale allows us to see an accurate representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-peeing",
   "metadata": {},
   "source": [
    "## LiDAR depth change vs InSAR Phase change Comparison\n",
    "The SnowEx 2020 campaign conducted a pair of LiDAR and InSAR flights over Grand Mesa on February 1st and 13th. The purpose of the paired data collected was to test the UAVSAR L-band InSAR SWE/Depth change technique against the LiDAR depth change retrievals. LiDAR is proven to work exceptionally well for measuring snow depth changes, so this provides an opportunity to validate the InSAR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://snowex-data/tutorial-data/sar/gmesa_depth_change_02-01_02-13.tif /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_dc = '/tmp/gmesa_depth_change_02-01_02-13.tif' #path to lidar depth change raster\n",
    "\n",
    "# print meta data, and check to see if the raster has a no data value\n",
    "lidar_rast = rio.open(lidar_dc)\n",
    "meta_data = lidar_rast.profile\n",
    "print(meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-logistics",
   "metadata": {},
   "source": [
    "##\n",
    "We can see this raster has a no data value of ```'nodata': -3.4e+38``` set (most of the UAVSAR ones did not). Therefore we can read it in with the ```masked=TRUE``` command to automatically mask out the no data pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.open(lidar_dc) as dataset:\n",
    "    lidar_masked = dataset.read(1, masked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test plot\n",
    "fig, ax = plt.subplots(figsize = (30,8))\n",
    "ax.set_title(\"LiDAR Depth Change\",fontsize = 16);\n",
    "show((lidar_masked), cmap = 'RdBu', vmin = -.3, vmax = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-boxing",
   "metadata": {},
   "source": [
    "## LiDAR vs UNW\n",
    "Using ```show_two_images()```, we can plot the LiDAR depth change and UNW images next to each other to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_two_images(lidar_masked, unw_array, 'RdBu', 'RdBu', 'LiDAR Depth Change (cm)', 'UNW (radians)', vmin1 = -.3, vmax1 = .3, vmin2=-4, vmax2=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-scanner",
   "metadata": {},
   "source": [
    "#\n",
    "The two rasters cover different areas, are differnt resolutions, and have different missing pixels. Let's zoom into the top left corner of Grand Mesa where there was significant wind drifting to compare the two data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_two_images(lidar_masked[700:1700,700:1700], unw_array[2150:2750,1300:1900], \n",
    "                'RdBu', 'RdBu', 'LiDAR Depth Change (cm)', 'UNW (radians)', vmin1 = -.3, vmax1 = .3, vmin2=-4, vmax2=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-collectible",
   "metadata": {},
   "source": [
    "#\n",
    "As you can see in the two plots above, there is a very strong spatial relationship between LiDAR depth change and InSAR change in phase. This relationship is the basis of measuring changes in SWE using L-band InSAR. While we don't get into the next step of converting phase change to SWE or depth change from InSAR, we will get into that in the UAVSAR project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
