{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hired-infrastructure",
   "metadata": {},
   "source": [
    "# Dynamic Query of SNOTEL data\n",
    "SnowEx Hackweek  \n",
    "July 13, 2021\n",
    "\n",
    "Author: David Shean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-compression",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will demonstrate a subset of basic concepts of geospatial data processing and analysis using data from the SNOTEL sensor network. We will demonstrate dynamic query of a public API to fetch point location data, review coordinate system transformations with GeoPandas, Geometry objects, and Pandas time series analysis/visualization.\n",
    "\n",
    "### Read a bit about SNOTEL data for the Western U.S.\n",
    "\n",
    "https://www.wcc.nrcs.usda.gov/snow/\n",
    "\n",
    "This is actually a nice web interface, with some advanced querying and interactive visualization.  You can also download formatted ASCII files (csv) for analysis.  This is great for one-time projects, but it's nice to have reproducible code that can be updated as new data appear, without manual steps.  That's what we're going to do here.\n",
    "\n",
    "#### About SNOTEL sites and data:\n",
    "* https://www.nrcs.usda.gov/wps/portal/wcc/home/aboutUs/monitoringPrograms/automatedSnowMonitoring\n",
    "* https://www.wcc.nrcs.usda.gov/snotel/snotel_sensors.html\n",
    "* https://directives.sc.egov.usda.gov/OpenNonWebContent.aspx?content=27630.wba\n",
    "\n",
    "#### Sample plots for SNOTEL site at Paradise, WA (south side of Mt. Rainier)\n",
    "* https://www.nwrfc.noaa.gov/snow/snowplot.cgi?AFSW1\n",
    "* We will reproduce some of these plots/metrics in this tutorial\n",
    "\n",
    "#### Interactive dashboard\n",
    "* https://climate.washington.edu/climate-data/snowdepth/\n",
    "\n",
    "#### Snow today\n",
    "* https://nsidc.org/reports/snow-today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-diary",
   "metadata": {},
   "source": [
    "## CUAHSI WOF server and automated Python data queries\n",
    "\n",
    "We are going to use a server set up by CUAHSI to serve the SNOTEL data, using a standardized database storage format and query structure.  You don't need to worry about this, but can quickly review the following:\n",
    "* http://hiscentral.cuahsi.org/pub_network.aspx?n=241 \n",
    "* http://his.cuahsi.org/wofws.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the latest CUAHSI API endpoint\n",
    "wsdlurl = 'https://hydroportal.cuahsi.org/Snotel/cuahsi_1_1.asmx?WSDL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-politics",
   "metadata": {},
   "source": [
    "### Acronym soup\n",
    "* SNOTEL = Snow Telemetry\n",
    "* CUAHSI = Consortium of Universities for the Advancement of Hydrologic Science, Inc\n",
    "* WOF = WaterOneFlow\n",
    "* WSDL = Web Services Description Language\n",
    "* USDA = United States Department of Agriculture\n",
    "* NRCS = National Resources Conservation Service\n",
    "* AWDB = Air-Water Database\n",
    "\n",
    "### Python options\n",
    "\n",
    "There are a few packages out there that offer convenience functions to query the online SNOTEL databases and unpack the results.  \n",
    "* climata (https://pypi.org/project/climata/) - last commit Sept 2017 (not a good sign)\n",
    "* ulmo (https://github.com/ulmo-dev/ulmo) - maintained by [Emilio Mayorga](https://apl.uw.edu/people/profile.php?last_name=Mayorga&first_name=Emilio) over at UW APL)\n",
    "\n",
    "You can also write your own queries using the Python `requests` module and some built-in XML parsing libraries.\n",
    "\n",
    "Hopefully not overwhelming amount of information - let's just go with ulmo for now.  I've done most of the work to prepare functions for querying and processing the data.  Once you wrap your head around all of the acronyms, it's pretty simple, basically running a few functions here: https://ulmo.readthedocs.io/en/latest/api.html#module-ulmo.cuahsi.wof\n",
    "\n",
    "We will use ulmo with daily data for this exercise, but please feel free to experiment with hourly data, other variables or other approaches to fetch SNOTEL data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import contextily as ctx\n",
    "import ulmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-mercy",
   "metadata": {},
   "source": [
    "## Part 1: Spatial Query SNOTEL sites\n",
    "* Use the ulmo cuahsi interface and the `get_sites` function to fetch available site metadata from server\n",
    "* This will return a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ulmo.cuahsi.wof.get_sites(wsdlurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview first item in dictionary\n",
    "next(iter(sites.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-marriage",
   "metadata": {},
   "source": [
    "### Store the dictionary as a Pandas DataFrame called `sites_df`\n",
    "* See the Pandas `from_dict` function\n",
    "* Use `orient` option so the sites comprise the DataFrame index, with columns for 'name', 'elevation_m', etc\n",
    "* Use the `dropna` method to remove any empty records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df = pd.DataFrame.from_dict(sites, orient='index').dropna()\n",
    "sites_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-sugar",
   "metadata": {},
   "source": [
    "### Clean up the DataFrame and prepare Point geometry objects\n",
    "* Convert `'location'` column (contains dictionary with `'latitude'` and `'longitude'` values) to Shapely `Point` objects\n",
    "* Store as a new `'geometry'` column (needed by GeoPandas)\n",
    "* Drop the `'location'` column, as this is no longer needed\n",
    "* Update the `dtype` of the `'elevation_m'` column to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df['geometry'] = [Point(float(loc['longitude']), float(loc['latitude'])) for loc in sites_df['location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df = sites_df.drop(columns='location')\n",
    "sites_df = sites_df.astype({\"elevation_m\":float})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-fiction",
   "metadata": {},
   "source": [
    "### Review output\n",
    "* Take a moment to familiarize yourself with the DataFrame structure and different columns.\n",
    "* Note that the index is a set of strings with format 'SNOTEL:1000_OR_SNTL'\n",
    "* Extract the first record with `loc`\n",
    "    * Review the `'site_property'` dictionary - could parse this and store as separate fields in the DataFrame if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df.loc['SNOTEL:301_CA_SNTL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df.loc['SNOTEL:301_CA_SNTL']['site_property']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-disorder",
   "metadata": {},
   "source": [
    "### Convert to a Geopandas GeoDataFrame\n",
    "* We already have `'geometry'` column, but still need to define the `crs` of the point coordinates\n",
    "* Note the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_gdf_all = gpd.GeoDataFrame(sites_df, crs='EPSG:4326')\n",
    "sites_gdf_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_gdf_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-cross",
   "metadata": {},
   "source": [
    "### Create a scatterplot showing elevation values for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#geojson of state polygons\n",
    "states_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\n",
    "states_gdf = gpd.read_file(states_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "sites_gdf_all.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\n",
    "#This prevents matplotlib from updating the axes extent (states polygons cover larger area than SNOTEL points)\n",
    "ax.autoscale(False)\n",
    "states_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-characteristic",
   "metadata": {},
   "source": [
    "### Exclude the Alaska (AK) points to isolate points over Western U.S.\n",
    "* Simple appraoch is to remove points where the site name contains 'AK' with attribute filter\n",
    "* Note the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_gdf_conus = sites_gdf_all[~(sites_gdf_all.index.str.contains('AK'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-quarterly",
   "metadata": {},
   "source": [
    "* Alternatively, can use a spatial filter (see GeoPandas `cx` indexer functionality for a bounding box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xmin, xmax, ymin, ymax = [-126, 102, 30, 50]\n",
    "#sites_gdf_conus = sites_gdf_all.cx[xmin:xmax, ymin:ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_gdf_conus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-examination",
   "metadata": {},
   "source": [
    "### Update your scatterplot as sanity check\n",
    "* Should look something like the Western U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "sites_gdf_conus.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\n",
    "ax.autoscale(False)\n",
    "states_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-healthcare",
   "metadata": {},
   "source": [
    "### Export SNOTEL site GeoDataFrame as a geojson\n",
    "* Maybe useful for other purposes, and can avoid all of the above processing, just load directly with geopandas `read_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_gdf_conus.to_file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_fn = 'snotel_conus_sites.json'\n",
    "if not os.path.exists(sites_fn):\n",
    "    sites_gdf_conus.to_file(sites_fn, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-newark",
   "metadata": {},
   "source": [
    "## Part 2: Spatial filter points by polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-norway",
   "metadata": {},
   "source": [
    "### Load Grand Mesa Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly_fn = 'grand_mesa_poly.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly_gdf = gpd.read_file(gm_poly_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-lover",
   "metadata": {},
   "source": [
    "## A quick aside on `geometry` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-greene",
   "metadata": {},
   "source": [
    "### Vector data contain `geometry` objects\n",
    "* Classes (Point, Line, Polygon) with unique attribute and methods\n",
    "* Polygon vs. MultiPolygon\n",
    "* https://automating-gis-processes.github.io/site/notebooks/L1/geometric-objects.html\n",
    "* https://shapely.readthedocs.io/en/stable/manual.html#geometric-objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-trust",
   "metadata": {},
   "source": [
    "![Geometry types](https://datacarpentry.org/organization-geospatial/fig/dc-spatial-vector/pnt_line_poly.png)\n",
    "Image Source: National Ecological Observatory Network (NEON), from https://datacarpentry.org/organization-geospatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-england",
   "metadata": {},
   "source": [
    "### Isolate Polygon geometry within GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gm_poly_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly_gdf.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly_gdf.iloc[0] #Now a GeoSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly_geom = gm_poly_gdf.iloc[0].geometry\n",
    "gm_poly_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gm_poly_geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(gm_poly_geom.exterior.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly_geom.bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-priority",
   "metadata": {},
   "source": [
    "### Generate boolean index for points that intersect the polygon\n",
    "* This will return a new GeoDataSeries with True/False values for each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = sites_gdf_all.intersects(gm_poly_geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-fifteen",
   "metadata": {},
   "source": [
    "### Use fancy indexing to isolate points and return new GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_snotel_sites = sites_gdf_all.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_snotel_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-upset",
   "metadata": {},
   "source": [
    "### Quick plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "gm_snotel_sites.plot(ax=ax, column='elevation_m', markersize=20, edgecolor='k', cmap='inferno', \\\n",
    "                  legend=True, legend_kwds={'label':'Elevation (m)'})\n",
    "#ctx.add_basemap(ax=ax, crs=gm_snotel_sites.crs, source=ctx.providers.Stamen.Terrain)\n",
    "ax.set_title('Grand Mesa SNOTEL Stations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "from geoviews import tile_sources as gvts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_snotel_sites.hvplot(hover_cols=['index','name']) * gm_poly_gdf.to_crs(gm_snotel_sites.crs).hvplot(color='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-alexander",
   "metadata": {},
   "source": [
    "### Add a basemap\n",
    "This functionality will add a publicly available tiled basemap to your interactive holoviews plot, which is useful for context during interactive visualization.\n",
    "\n",
    "Note that we need to reproject our GeoDataframe to match the CRS of the tiled basemap here.\n",
    "\n",
    "For static plots with matplotlib (e.g., for publication) can also use `contextily` for this https://contextily.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tiles = gvts.EsriImagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tiles * gm_snotel_sites.to_crs('EPSG:3857').hvplot(hover_cols=['index','name']) * \\\n",
    "gm_poly_gdf.to_crs('EPSG:3857').hvplot(color='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-tsunami",
   "metadata": {},
   "source": [
    "## Part 3: Time series analysis for one station\n",
    "Now that we've identified sites of interest, let's query the API to obtain the time series data for variables of interest (snow!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-creation",
   "metadata": {},
   "source": [
    "https://wcc.sc.egov.usda.gov/nwcc/site?sitenum=622&state=co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitecode = gm_snotel_sites.index[-1]\n",
    "sitecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-watson",
   "metadata": {},
   "source": [
    "### Get available measurements for this site\n",
    "Note that there are many standard meteorological variables that can be downloaded for this site, in addition to the snow depth and snow water equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "ulmo.cuahsi.wof.get_site_info(wsdlurl, sitecode)['series'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-queens",
   "metadata": {},
   "source": [
    "* _H = \"hourly\"\n",
    "* _D = \"daily\"\n",
    "* _sm, _m = \"monthly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-thought",
   "metadata": {},
   "source": [
    "### Let's consider the 'SNOTEL:SNWD_D' variable (Daily Snow Depth)\n",
    "* Assign 'SNOTEL:SNWD_D' to a variable named `variablecode`\n",
    "* Get some information about the variable using `get_variable_info` method\n",
    "    * Note the units, nodata value, etc.\n",
    "* **Note: The snow depth records are almost always shorter/noisier than the SWE records for SNOTEL sites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daily SWE\n",
    "#variablecode = 'SNOTEL:WTEQ_D'\n",
    "#Daily snow depth\n",
    "variablecode = 'SNOTEL:SNWD_D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hourly SWE\n",
    "#variablecode = 'SNOTEL:WTEQ_H'\n",
    "#Hourly snow depth\n",
    "#variablecode = 'SNOTEL:SNWD_H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "ulmo.cuahsi.wof.get_variable_info(wsdlurl, variablecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-friday",
   "metadata": {},
   "source": [
    "### Define a function to fetch data\n",
    "* I've done this for you, but please review the comments and steps to see what is going on under the hood\n",
    "* You'll probably have to do similar data wrangling for another project at some point in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get current datetime\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "def snotel_fetch(sitecode, variablecode='SNOTEL:SNWD_D', start_date='1950-10-01', end_date=today):\n",
    "    #print(sitecode, variablecode, start_date, end_date)\n",
    "    values_df = None\n",
    "    try:\n",
    "        #Request data from the server\n",
    "        site_values = ulmo.cuahsi.wof.get_values(wsdlurl, sitecode, variablecode, start=start_date, end=end_date)\n",
    "        #Convert to a Pandas DataFrame   \n",
    "        values_df = pd.DataFrame.from_dict(site_values['values'])\n",
    "        #Parse the datetime values to Pandas Timestamp objects\n",
    "        values_df['datetime'] = pd.to_datetime(values_df['datetime'], utc=True)\n",
    "        #Set the DataFrame index to the Timestamps\n",
    "        values_df = values_df.set_index('datetime')\n",
    "        #Convert values to float and replace -9999 nodata values with NaN\n",
    "        values_df['value'] = pd.to_numeric(values_df['value']).replace(-9999, np.nan)\n",
    "        #Remove any records flagged with lower quality\n",
    "        values_df = values_df[values_df['quality_control_level_code'] == '1']\n",
    "    except:\n",
    "        print(\"Unable to fetch %s\" % variablecode)\n",
    "\n",
    "    return values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-construction",
   "metadata": {},
   "source": [
    "### Use this function to get the full 'SNOTEL:SNWD_D' record for one station\n",
    "* Inspect the results\n",
    "* We used a dummy start date of Jan 1, 1950.  What is the actual the first date returned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all records, can filter later\n",
    "start_date = datetime(1950,1,1)\n",
    "end_date = datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sitecode)\n",
    "values_df = snotel_fetch(sitecode, variablecode, start_date, end_date)\n",
    "values_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of decimal years between first and last observation\n",
    "nyears = (values_df.index.max() - values_df.index.min()).days/365.25\n",
    "nyears"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-belarus",
   "metadata": {},
   "source": [
    "### Create a quick plot to view the time series\n",
    "* Take a moment to inspect the `value` column, which is where the `SNWD_D` values are stored\n",
    "* Sanity check thought question: *What are the units again?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-conducting",
   "metadata": {},
   "source": [
    "### Compute the integer day of year (doy) and integer day of water year (dowy)\n",
    "* Can get doy for each record with `df.index.dayofyear`\n",
    "    * Can compute on the fly, but add a new column to store these values\n",
    "    * https://pandas.pydata.org/pandas-docs/version/0.19/generated/pandas.DatetimeIndex.dayofyear.html\n",
    "* For the day of water year, you'll need to offset by 9 months, then compute day of year\n",
    "    * https://en.wikipedia.org/wiki/Water_year\n",
    "    * Add another column to store these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add DOY and DOWY column\n",
    "#Need to revisit for leap year support\n",
    "def add_dowy(df, col=None):\n",
    "    if col is None:\n",
    "        df['doy'] = df.index.dayofyear\n",
    "    else:\n",
    "        df['doy'] = df[col].dayofyear\n",
    "    # Sept 30 is doy 273\n",
    "    df['dowy'] = df['doy'] - 273\n",
    "    df.loc[df['dowy'] <= 0, 'dowy'] += 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dowy(values_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-denial",
   "metadata": {},
   "source": [
    "### Compute statistics for each day of the water year, using values from all years\n",
    "* Seems like a Pandas groupby/agg might work here\n",
    "* Stats should at least include min, max, mean, and median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_list = ['count','min','max','mean','std','median','mad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "doy_stats = values_df.groupby('dowy').agg(stat_list)['value']\n",
    "doy_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-density",
   "metadata": {},
   "source": [
    "### Create a plot of these aggregated dowy values\n",
    "* Something like the 30-year mean and median here: https://www.nwrfc.noaa.gov/snow/plot_SWE.php?id=AFSW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "for stat in ['min','max','mean','median']:\n",
    "    ax.plot(doy_stats.index, doy_stats[stat], label=stat)\n",
    "\n",
    "ax.fill_between(doy_stats.index, doy_stats['mean'] - doy_stats['std'], doy_stats['mean'] + doy_stats['std'], \\\n",
    "                color='lightgrey', label='1-std')\n",
    "\n",
    "title = f'{sitecode}: \\n{values_df.index.min().date()} to {values_df.index.max().date()} ({nyears:.2f} years)'\n",
    "\n",
    "ax.set_title(title)\n",
    "ax.set_xlabel('Day of Water Year (Oct 1)')\n",
    "ax.set_ylabel('Snow depth (in)')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_xlim(0,366)\n",
    "ax.set_ylim(bottom=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-effects",
   "metadata": {},
   "source": [
    "### Add the daily snow depth values for the current water year\n",
    "* Can use pandas indexing here with simple strings ('YYYY-MM-DD'), or Timestamp objects\n",
    "    * Standard slicing also works with `:`\n",
    "* Make sure to `dropna` to remove any records missing data\n",
    "* Add this to your plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variable to store current year\n",
    "curr_y = datetime.now().year\n",
    "curr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_wy = values_df.loc['2019-10-1':].dropna()\n",
    "df_wy = values_df.loc[f'{curr_y-1}-10-1':].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10,5))\n",
    "for stat in ['min','max','mean','median']:\n",
    "    ax.plot(doy_stats.index, doy_stats[stat], label=stat)\n",
    "\n",
    "ax.fill_between(doy_stats.index, doy_stats['mean'] - doy_stats['std'], doy_stats['mean'] + doy_stats['std'], \\\n",
    "                color='lightgrey', label='1-std')\n",
    "\n",
    "ax.set_title(title)\n",
    "ax.set_xlabel('Day of Water Year')\n",
    "ax.set_ylabel('Snow depth (in)')\n",
    "ax.grid()\n",
    "ax.plot(df_wy['dowy'], df_wy['value'], marker='.', color='k', ls='none', label='Current WY')\n",
    "ax.legend()\n",
    "ax.set_xlim(0,366)\n",
    "ax.set_ylim(bottom=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-swaziland",
   "metadata": {},
   "source": [
    "### What was the percentage of \"normal\" snow depth on April 1 of this year?\n",
    "* Can use long-term median for this dowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wy.loc[f'{curr_y}-4-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "apr1_doy = df_wy.loc[f'{curr_y}-4-1']['dowy']\n",
    "apr1_doy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "doy_stats.loc[apr1_doy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percent of normal\n",
    "perc_normal = 100 * df_wy.loc[f'{curr_y}-4-1']['value']/doy_stats.loc[apr1_doy]['median']\n",
    "print(f'{perc_normal:0.2f}% percent of normal snow depth on DOWY {apr1_doy} in {curr_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-situation",
   "metadata": {},
   "source": [
    "### Index DataFrame by date or date range\n",
    "* Maybe useful for project work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = '2017-2-1'\n",
    "dt2 = '2017-2-4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single date\n",
    "values_df.loc[dt1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.loc[dt1]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.loc[dt1:dt2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-samuel",
   "metadata": {},
   "source": [
    "### Query multiple sites\n",
    "* Can use a loop to query multiple sites and/or variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define an empty dictionary to store returns for each site\n",
    "value_dict = {}\n",
    "for i, sitecode in enumerate(gm_snotel_sites.index):\n",
    "    print('%i of %i sites: %s' % (i+1, len(gm_snotel_sites.index), sitecode))\n",
    "    out = snotel_fetch(sitecode, variablecode)\n",
    "    if out is not None:\n",
    "        value_dict[sitecode] = out['value']\n",
    "#Convert the dictionary to a DataFrame, automatically handles different datetime ranges (nice!)\n",
    "multi_df = pd.DataFrame.from_dict(value_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df.hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hack to remove bad measurements\n",
    "multi_df[multi_df > 190] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-acquisition",
   "metadata": {},
   "source": [
    "### Scatterplot to compare corresponding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df.dropna().hvplot(kind='scatter', x='SNOTEL:622_CO_SNTL', y='SNOTEL:682_CO_SNTL', \\\n",
    "                         aspect='equal', s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-reservation",
   "metadata": {},
   "source": [
    "### Determine Pearson's correlation coefficient for the two time series\n",
    "* https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
    "* See the Pandas `corr` method\n",
    "    * This should properly handle nan under the hood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-disabled",
   "metadata": {},
   "source": [
    "Highly correlated snow depth records for these two sites!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
