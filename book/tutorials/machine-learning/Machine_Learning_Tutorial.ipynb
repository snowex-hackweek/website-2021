{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "requested-victor",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning \n",
    "<center> Compiled by: </center>\n",
    "<center> Ibrahim O. Alabi (Computing PhD, Data Science, Boise State University) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-honduras",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "1. Understand the goals and main concepts of a Machine Learning Algorithm\n",
    "2. Prepare a SnowEx dataset for Machine Learning\n",
    "3. Understand the fundamental types and techniques in Machine Learning\n",
    "4. Implement Machine Learning with a SnowEx dataset\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "Machine Learning simply means building algorithms or computer models using data. The goal is to use these \"trained\" computer models to make decisions. \n",
    "\n",
    "Here is a general definition;\n",
    "\n",
    "> Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959)\n",
    "\n",
    "Over the years, ML algorithms have achieved great success in a wide variety of fields. Its success stories include disease diagnostics, image recognition, self-driving cars, spam detectors, and handwritten digit recognition. In this tutorial we will train a model using a SnowEx dataset.\n",
    "\n",
    "### Binary Example\n",
    "\n",
    "Suppose we want to build a computer model that returns 1 if the input is a prime number and 0 otherwise. This model may be represented as follows;\n",
    "\n",
    "$$Y = F(X)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "   * $X \\to$ the number entered also called a feature\n",
    "   * $Y \\to$ the outcome we want to predict\n",
    "   * $F \\to$ the model that gets the job done\n",
    "\n",
    "Contrary to classical programming where we the program function, in Machine Learning we learn the function by training the algorithm with data. \n",
    "\n",
    "<img src=\"images/ml.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad$ image by [Kpaxs on Twitter](https://twitter.com/Kpaxs/status/1163058544402411520)\n",
    "\n",
    "Machine Learning is useful when the function cannot be programmed or the relationship between the features and outcome is unknown. \n",
    "\n",
    "## SnowEx Data\n",
    "\n",
    "Now we will import airborned data from 2017 SnowEx Campaign. In Machine Learning terminologies, it contains the following features;\n",
    "\n",
    "- phase\n",
    "- coherence\n",
    "- amplitude\n",
    "- incidence angle\n",
    "\n",
    "and outcome\n",
    "\n",
    "- snow depth\n",
    "\n",
    "The goal is to use the data to learn the computer model $f$ so that\n",
    "\n",
    "snow_depth = $f$(phase, coherence, amplitude, incidence angle) \n",
    "\n",
    "Once $f$ is learned, it can be used to predict snow depth given the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-gallery",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Note that this dataset has been cleaned in a separate notebook, and it is available for anyone interested.\n",
    "\n",
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/snow_depth_data.csv\")\n",
    "dataset.info()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-hobby",
   "metadata": {},
   "source": [
    "The data used in this tutorial is already clean. The data cleaning was done in a separate notebook, and it is available for anyone interested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-bradley",
   "metadata": {},
   "source": [
    "## Train and Test Sets\n",
    "\n",
    "For the algorithm to learn the relationship pattern between the feature(s) and the outcome variable, it has to be exposed to examples. The dataset containing the examples for training a learning machine is called the *train set* ($\\mathcal{D}^{(tr)}$). \n",
    "\n",
    "On the other hand, the accuracy of an algorithm is measured on how well it predicts the outcome of observations it has not seen before. The dataset containing the observations not used in training the ML algorithm is called the *test set* ($\\mathcal{D}^{(te)}$). \n",
    "\n",
    "In practice, we divide our dataset into train and test sets, train the algorithm on the train set and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-kruger",
   "metadata": {},
   "source": [
    "### Inspect the Data\n",
    "\n",
    "**Visualization**\n",
    "\n",
    "Before modelling, it is always a good idea to visualize our dataset. With visualization, we gain insights into the relationships between the variables and the shape of the distribution of each variable. For this data, we shall look into the scatterplot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_dataset[['snow_depth','amplitude', 'coherence', 'phase', 'inc_ang']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-korea",
   "metadata": {},
   "source": [
    "Each panel of the scatterplot matrix is a scatterplot for a pair of variables whose identities are given by the corresponding row and column labels. None of the features have a linear relationship with $\\texttt{snow_depth}$. This may indicate that a linear model might not be the best option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-magnet",
   "metadata": {},
   "source": [
    "**Descriptive Statistics**\n",
    "\n",
    "- count: the size of the training set\n",
    "- mean: arithmetic mean\n",
    "- std: sample standard deviation\n",
    "- min: minimum value\n",
    "- 25%: 25$^{th}$ percentile\n",
    "- 50%: 50$^{th}$ percentile also called the median\n",
    "- 75%: 75$^{th}$ percentile\n",
    "- max: maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-apartment",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "The features and outcome are measured in different units and hence are on different scales. Machine Learning algorithms are very sensitive and important information can be lost if they are not on the same scale A simple way to address this is to project all the variables onto the same scale, in a process known as **Normalization**.\n",
    "\n",
    "Normalization simply consists of transforming the variables such that all values are in the unit interval [0, 1]. With Normalization, if $X_j$ is one of the variables, and we have $n$ observations $X_{1j}, X_{2j}, \\cdots, X_{nj}$, then the normalized version of $X_{ij}$ is given by\n",
    "\n",
    "$$\n",
    "\\tilde{X}{ij} = \\frac{X_{ij} - \\min X_j}{\\max X_j - \\min X_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_train = scaler.fit_transform(train_dataset)\n",
    "scaled_test = scaler.transform(test_dataset) ## fit_transform != transform. \n",
    "                                             ## transform uses the parameters of fit_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-physiology",
   "metadata": {},
   "source": [
    "### Sepatare Features from Labels\n",
    "\n",
    "Our last dtep for preparing the data for Machine Learning is to separate the features from the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = scaled_train[:, :-1], scaled_train[:, -1]\n",
    "test_X, test_y = scaled_test[:, :-1], scaled_test[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-deputy",
   "metadata": {},
   "source": [
    "## Why Estimate $f$?\n",
    "\n",
    "We estimate $f$ for two main reasons;\n",
    "\n",
    "1. Prediction: in this case, the features $X$ are available, but there is no explicit rule for obtaining the outcome $Y$.\n",
    "2. Inference: in practice, we are sometimes interested in how changing the input $X$ effects $Y$. Inference can tell us which features are significantly associated with a particular outcome and the nature of the relationship between $X$ and $Y$.\n",
    "    \n",
    "\n",
    "## How Do We Estimate $f$?\n",
    "\n",
    "### Machine Learning Algorithms\n",
    "\n",
    "Machine learning algorithms can be categorized based on different criteria. In this tutorial, our categorization will be based on the amount and type of supervision needed during the training process. Based on this criterion, there are four major categories; supervised learning, unsupervised learning, semisupervised learning, and reinforcement learning. We shall limit our definition to the first two;\n",
    "\n",
    "* **Supervised Learning**: this refers to tasks where we have a specific outcome to predict. That is, every observation of the features has a corresponding outcome. An example of a supervised learning task is predicting snow depth based on some influencing features.\n",
    "\t \n",
    "* **Unsupervised Learning**: this refers to tasks where we have no outcome to predict.  Here, rather than predict an outcome, we seek to understand the relationship between the features or between the observations or to detect anomalous observations. Considering the example above, we assume the snow depth variable does not exist and we either understand the relationship between the features or between the observations based on the features.\n",
    "\n",
    "\n",
    "It is worth noting that a variable can either be categorical or continuous. For now, let's focus on the nature of the outcome variable. In *Supervised Learning* parlance, if the outcome variable is categorical, we have a *classification* task and if continuous, we are in the presence of a *regression* task. Categorical implies that the variable is made of distinct categories (e.g. hair color: grey, blonde, black) and continuous implies that the variable is measured (e.g. snow depth). For the rest of this tutorial, we will focus on *Supervised Learning* tasks with a special concentration on regression tasks.\n",
    "\n",
    "\n",
    "## Machine Learning Installation Guide\n",
    "\n",
    "For this notebook to run successfully, you have to install the following packages;\n",
    "\n",
    "1. TensorFlow\n",
    "2. Keras\n",
    "\n",
    "**Tensorflow**: it is an end-to-end open source platform for machine learning. It makes it easy to train and deploy machine learning models. TensorFlow was created at Google and supports many of their large-scale Machine Learning applications. Read more at [About TensorFlow](https://keras.io/about/)\n",
    "\n",
    "**Keras:**  Keras is a deep learning Application Programming Interface (API) written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Read more at [About Keras](https://www.tensorflow.org/).\n",
    "\n",
    "Other packages needed come pre-installed in Anaconda\n",
    "\n",
    "The install the packages above, lunch the **Anaconda prompt** and type the following commands\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  $\\texttt{C:\\Users\\Default> pip install tensorflow}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  $\\texttt{C:\\Users\\Default> pip install keras}$\n",
    "\n",
    "After installation, you will be able to continue with the tutorial. For this tutorial, both Tensorflow and Keras used are version 2.5\n",
    "\n",
    "\n",
    "Most Machine Learning techniques can be characterised as either *parametric* or *non-parametric*. A common parametric method is Linear Regression while Neural Networks are a common non-parametric method.\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "Assume a functional form for $f$, e.g we may assume that $f$ is a linear function of $X$:\n",
    "\n",
    "$$f(X) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_k X_k$$\n",
    "\n",
    "and the goal of ML is to estimate the parameters of the model $\\beta_0, \\beta_1, \\cdots, \\beta_k$. This is called Linear Regression.\n",
    "\n",
    "The parametric approach simplifies the problem of estimating $f$ to a parameter estimation problem. The disadvantage is that we assume a particular shape of $f$ that may not match the true shape of $f$. The major advantage of using parametric approach that when inference is the goal we can understand how changing $X_1, X_2, \\cdots, X_k$ effects $Y$\n",
    "\n",
    "\n",
    "## Performance Evaluation\n",
    "\n",
    "Each time we estimate the true outcome ($Y$) using a trained ML algorithm ($f(X)$), the discrepancy between the observed and predicted must be quantified. The question is, how do we quantify this discrepancy? This brings the notion of **loss function**. \n",
    "\n",
    "*Loss Function* $\\mathcal{L} (\\cdot,\\cdot)$ is a bivariate function that quantifies the loss (error) we sustain from predicting $Y$ with $f(X)$. Put another way, **loss function** quantifies how close the prediction $f(X)$ is to the ground truth $Y$.\n",
    "\n",
    "* Regression Loss Function\n",
    "\n",
    "There exists quite a number of ways for which the loss of a regression problem may be quantified. We now illustrate two of them;\n",
    "\n",
    "1. \n",
    "\n",
    "$$\n",
    "\\mathcal{L} (Y,f(X)) = (Y - f(X))^2\n",
    "$$\n",
    "\n",
    "This is popularly known as the *squared error loss* and it is simply the square of the difference between the observed and the predicted values. The loss is squared so that the function reaches its minimum (convex).\n",
    "\n",
    "2.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} (Y,f(X)) = |Y - f(X)|\n",
    "$$\n",
    "\n",
    "Another way to quantify regression loss is by taking the absolute value of the difference between the observed ($Y$) and the predicted ($f(X)$) values. This is called the L1 loss.\n",
    "\n",
    "It is worth noting that the *loss function* as defined above corresponds to a single observation. However, in practice, we want to quantify the loss over the entire dataset and this is where the notion of **empirical risk** comes in. Loss quantified over the entire dataset is called the *empirical risk*. Our goal in ML is to develop an algorithm such that the *empirical risk* is as minimum as possible. *Empirical risk* is also called the *cost function* or the *objective function* we want to minimize.\n",
    "\n",
    "* Regression Empirical Risk\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathcal{R}}_n(f) = \\frac{1}{n}\\sum_{i=1}^n{\\mathcal{L}(Y_i, f(X_i))}\n",
    "$$\n",
    "\n",
    "The empirical risk corresponding to the squared error loss is called \"mean squared error\", while the empirical risk corresponding to the L1 loss is called \"mean absolute error\". Other Regression Loss functions can be found at [Keras: losses](https://keras.io/api/losses/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-antenna",
   "metadata": {},
   "source": [
    "## Modeling Setup\n",
    "\n",
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this part of the tutorial uses additional libraries not in the default snowex jupyterhub\n",
    "# mamba is a python package management alternative to conda and pip https://github.com/mamba-org/mamba\n",
    "!mamba install -y -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # Keras is part of standard tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check the version of Tensorflow and Keras\n",
    "print(\"TensorFlow version ==>\", tf.__version__) \n",
    "print(\"Keras version ==>\",tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-mixer",
   "metadata": {},
   "source": [
    "The machine learning algorithm uses a linear regression model to fit the features to the outcome. It will initialize different weights depending on the seed. We define a random seed so that we get same result each time we do the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0) ## For reproducible results\n",
    "linear_regression = tf.keras.models.Sequential() # Specify layers in their sequential order\n",
    "# inputs are 4 dimensions (4 dimensions = 4 features)\n",
    "# Dense = Fully Connected.  \n",
    "linear_regression.add(tf.keras.layers.Dense(1, activation=None ,input_shape=(train_X.shape[1],)))\n",
    "# Output layer has no activation with just 1 node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-inspector",
   "metadata": {},
   "source": [
    "* **Compile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "linear_regression.compile(optimizer = opt, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-canyon",
   "metadata": {},
   "source": [
    "The mean squared error is minimized to find optimal parameters. A discussion of diffent optimization methods is provided in Appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-maintenance",
   "metadata": {},
   "source": [
    "* **Print Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear_regression.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-surge",
   "metadata": {},
   "source": [
    "Note since there are 4 features, we have 5 regression parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-greek",
   "metadata": {},
   "source": [
    "* **Fit Model**\n",
    "\n",
    "The regression parameters are formed with data in the training set. An epoch is when an entire batch of the training set has been used. The number of times we iterate over the dataset is known as the number of **epochs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# NOTE: can changed from epochs=150 to run faster, change to verbose=1 for per-epoch output\n",
    "history =linear_regression.fit(train_X, train_y, epochs=100, validation_split = 0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-crime",
   "metadata": {},
   "source": [
    "### Linear Regression Coefficient\n",
    "\n",
    "Note that all coefficients are from the transformed data. Therefore, we need to take the inverse transform before quantifying our error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-record",
   "metadata": {},
   "source": [
    "**model**: $\\texttt{snow_depth} = 0.18 - 0.31 \\texttt{amplitude} - 0.14 \\texttt{coherence} + 0.40\\texttt{phase} + 0.40\\texttt{inc_ang} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-aging",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural networks (NN) are a type of non-parametric method. Another non-parametric approach is Support Vector Machine (SVM) but we will not discuss that here. Non-parametric approaches do not assume any shape for $f$. Instead, the try to estimate $f$ that gets as close to the data points as possible. Neural networks are learning machines inspired by the functionality of biological neurons. They are not models of the brain, because there is no proven evidence that the brain operates the same way neural networks learn representations. However, some of the concepts were inspired by understanding the brain. Every NN consists of three distinct layers; \n",
    "\n",
    "1. Input layer \n",
    "2. Hidden layer(s) and \n",
    "3. Output layer. \n",
    "\n",
    "To understand the functioning of these layers, we begin by exploring the building blocks of neural networks; a single neuron or perceptron.\n",
    "\n",
    "### A preceptron or Single Neuron\n",
    "\n",
    "Each layer in a NN consists of small individual units called neurons (usually represented with a circle). A neuron receives inputs from other neurons, performs some mathematical operations, and then produces an output. Each neuron in the input layer represents a  feature. In essence, the number of neurons in the input layer equals the number of features. Each neuron in the input layer is connected to every neuron in the hidden layer. The number of neurons in the hidden layer is not fixed, it is problem dependent and it is often determined via cross-validation or validation set approach in practice. \n",
    "\n",
    "<img src=\"images/feedforward.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The configuration of the hidden layer(s) controls the complexity of the network. A NN may contain more than one hidden layer. A NN with more than one hidden layer is called a **deep neural network** while that with a single hidden layer is a **shallow neural network**. The final layer of a neural network is called the output layer and it holds the predicted outcomes of observations passed into the input layer. Every inter-neuron connection has an associated weight, these weights are what the neural network learns during the training process. When connections between the layers do not form a loop, such networks are called **feedforward neural networks** and if otherwise, they are called **recurrent neural networks**. In this tutorial, we shall concentrate on the feed forward neural networks.\n",
    "\n",
    "\n",
    "\n",
    "#### How the perceptron works\n",
    "\n",
    "Consider a dataset $\\mathcal{D}_n = \\left\\lbrace (\\textbf{x}_1, y_1), (\\textbf{x}_2, y_2), \\cdots, (\\textbf{x}_n, y_n) \\right\\rbrace$ where $\\textbf{x}_i^\\top \\equiv ({x}_{i1}, {x}_{i2}, \\cdots, {x}_{ik})$ denotes the $k$-dimensional vector of features, and $y_i$ represents the corresponding outcome. Given a set of input fed into the network through the input layer, the output of a neuron in the hidden layer is\n",
    "\n",
    "$$\n",
    " f(\\textbf{x}_i;\\textbf{w}) = g(w_0 + \\textbf{w}^\\top \\textbf{x}_i),\n",
    "$$\n",
    "\n",
    "where $\\textbf{w} = (w_1, w_2, \\cdots, w_k)^\\top$ is a vector of weights and  $w_0$ is the bias term associated with the neuron. The weights can be thought of as the slopes in a linear regression and the bias as the intercept. The function $g(\\cdot)$ is known as the activation function and it is  used to introduce non-linearity into the network.\n",
    "\n",
    "<img src=\"images/perceptron.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad$  A Perceptron\n",
    "\n",
    "There exists a number of activation functions in practice, the commonly used ones are\n",
    "\n",
    "1. Sigmoid or Logistic activation function $$g(z) = \\frac{1}{1 + e^{-z}}.$$\n",
    "2. Hyperbolic Tangent activation function $$g(z) = \\tanh (z) = \\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}.$$\n",
    "3. Rectified Linear Unit activation function $$g(z) = \\max(0,z).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-porter",
   "metadata": {},
   "source": [
    "### Visualizing Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(z):\n",
    "    \"\"\"\n",
    "    A function that performs the sigmoid transformation\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "        -* z: array/list of numbers to activate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        -* logistic: the transformed/activated version of the array\n",
    "    \"\"\"\n",
    "    \n",
    "    logistic = 1/(1+ np.exp(-z))\n",
    "    return logistic\n",
    "    \n",
    "\n",
    "def Tanh(z):\n",
    "    \"\"\"\n",
    "    A function that performs the hyperbolic tangent transformation\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "        -* z: array/list of numbers to activate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        -* hyp: the transformed/activated version of the array\n",
    "    \"\"\"\n",
    "    \n",
    "    hyp = np.tanh(z)\n",
    "    return hyp\n",
    "\n",
    "\n",
    "def ReLu(z):\n",
    "    \"\"\"\n",
    "    A function that performs the hyperbolic tangent transformation\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "        -* z: array/list of numbers to activate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        -* points: the transformed/activated version of the array\n",
    "    \"\"\"\n",
    "    \n",
    "    points = np.where(z < 0, 0, z)\n",
    "    return points\n",
    "\n",
    "z = np.linspace(-10,10)\n",
    "fa = plt.figure(figsize=(16,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(z,Sigmoid(z),color=\"red\", label=r'$\\frac{1}{1 + e^{-z}}$')\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('g(z)', fontsize=15)\n",
    "plt.title(\"Sigmoid Activation Function\")\n",
    "plt.legend(loc='best',fontsize = 22)\n",
    "\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(z,Tanh(z),color=\"red\", label=r'$\\tanh (z)$')\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('g(z)', fontsize=15)\n",
    "plt.title(\"Hyperbolic Tangent Activation Function\")\n",
    "plt.legend(loc='best',fontsize = 18)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(z,ReLu(z),color=\"red\", label=r'$\\max(0,z)$')\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('g(z)', fontsize=15)\n",
    "plt.title(\"Rectified Linear Unit Activation Function\")\n",
    "plt.legend(loc='best', fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-richards",
   "metadata": {},
   "source": [
    "### Families of Neural Networks\n",
    "\n",
    "1. Feedforward Neural Networks: often used for structural data\n",
    "2. Convolutional Neural Networks: the gold standard for image classification\n",
    "3. Transfer Learning: reusing knowledge from previous tasks on new tasks\n",
    "4. Recurrent Neural Networks: well suited for sequence data such as texts, time series, drawing generation\n",
    "5. Encoder-Decoders: comminly used for but not limited to machine translation\n",
    "6. Generative Adversarial Networks: usually used for 3D modelling for video games, animation, etc.\n",
    "7. Graph Neural Networks: usually used to work with graph data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-blackjack",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network\n",
    "\n",
    "* **Specify Architecture**\n",
    "\n",
    "Here, we shall include three hidden layers with 1000, 512, and 256 neurons respectively. Layers added using the \"$\\texttt{network.add($\\cdots$)}$\" command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1000)  ## For reproducible results\n",
    "network = tf.keras.models.Sequential() # Specify layers in their sequential order\n",
    "# inputs are 4 dimensions (4 dimensions = 4 features)\n",
    "# Dense = Fully Connected.   \n",
    "# First hidden layer has 1000 neurons with relu activations.\n",
    "# Second hidden layer has 512 neurons with relu activations\n",
    "# Third hidden layer has 256 neurons with Sigmoid activations\n",
    "network.add(tf.keras.layers.Dense(1000, activation='relu' ,input_shape=(train_X.shape[1],)))\n",
    "network.add(tf.keras.layers.Dense(512, activation='relu')) # sigmoid, tanh\n",
    "network.add(tf.keras.layers.Dense(256, activation='sigmoid'))\n",
    "# Output layer uses no activation with 1 output neurons\n",
    "network.add(tf.keras.layers.Dense(1)) # Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-consistency",
   "metadata": {},
   "source": [
    "* **Compile**\n",
    "\n",
    "The learning rate controls how the weights and bias are optimized. A discussion of the optimization procedure for Neural Networks can be found in Appendix B. If the learning rate is too small the training may get stuck, while if it is too big the trained model may be unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "network.compile(optimizer = opt, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-indonesia",
   "metadata": {},
   "source": [
    "* **Print Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-finder",
   "metadata": {},
   "source": [
    "Number of weights connecting the input and the first hidden layer = (1000 $\\times$ 4) + 1000(bias) = 5000 \n",
    "\n",
    "* 4 = $\\texttt{number of features}$\n",
    "* 1000 = $\\texttt{number of nodes in the first hidden layer}$\n",
    "\n",
    "Number of weights connecting the first hidden layer and the second hidden layer = (1000 $\\times$ 512) + 512(bias) = 512512 \n",
    "\n",
    "* 512 = $\\texttt{number of nodes in the second hidden layer}$\n",
    "* 1000 = $\\texttt{number of nodes in the first hidden layer}$\n",
    "\n",
    "Number of weights connecting the second hidden layer and the third hidden layer = (512 $\\times$ 256) + 256(bias) = 131328 \n",
    "\n",
    "* 256 = $\\texttt{number of nodes in the third hidden layer}$\n",
    "* 512 = $\\texttt{number of nodes in the second hidden layer}$\n",
    "\n",
    "Number of weights connecting the third (last) hidden and the output layers = (256 $\\times$ 1) + 1(bias) = 257 \n",
    "\n",
    "* 256 = $\\texttt{number of nodes in the third hidden layer}$\n",
    "* 1 = $\\texttt{number of nodes in the output layer}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-compiler",
   "metadata": {},
   "source": [
    "* **Fit Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# NOTE: if you have time, consider upping epochs -> 150\n",
    "history =network.fit(train_X, train_y, epochs=20, validation_split = 0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-dragon",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "We have used Machine Learning to build two computer models $f$ with SnowEx data. Now we will use the computer models to estimate snow depth for a given set of features (phase, coherence, amplitude, incidence angle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear Regression\n",
    "\n",
    "yhat_linReg = linear_regression.predict(test_X)\n",
    "inv_yhat_linReg = np.concatenate((test_X, yhat_linReg), axis=1)\n",
    "inv_yhat_linReg = scaler.inverse_transform(inv_yhat_linReg)\n",
    "inv_yhat_linReg = inv_yhat_linReg[:,-1]\n",
    "\n",
    "## DNN\n",
    "yhat_dnn = network.predict(test_X) \n",
    "inv_yhat_dnn = np.concatenate((test_X, yhat_dnn), axis=1)\n",
    "inv_yhat_dnn = scaler.inverse_transform(inv_yhat_dnn)\n",
    "inv_yhat_dnn = inv_yhat_dnn[:,-1]\n",
    "\n",
    "## True Snow Depth (Test Set)\n",
    "inv_y = test_dataset[\"snow_depth\"]\n",
    "\n",
    "\n",
    "## Put Observed and Predicted (Linear Regression and DNN) in a Dataframe\n",
    "prediction_df = pd.DataFrame({\"Observed\": inv_y,\n",
    "                    \"LR\":inv_yhat_linReg, \"DNN\":inv_yhat_dnn})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-repair",
   "metadata": {},
   "source": [
    "### Check Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_print(test_data,test_predict):\n",
    "    print('Test RMSE: ', round(np.sqrt(mean_squared_error(test_data, test_predict)), 2))\n",
    "    print('Test R^2 : ', round((r2_score(test_data, test_predict)*100), 2) ,\"%\")\n",
    "    print('Test MAPE: ', round(mean_absolute_percentage_error(test_data, test_predict)*100,2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##************** Linear Regression Results **************##\")\n",
    "metrics_print(prediction_df['Observed'], prediction_df['LR'])\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "print(\"##************** Deep Learning Results **************##\")\n",
    "metrics_print(prediction_df['Observed'], prediction_df['DNN'])\n",
    "print(\" \")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-lightning",
   "metadata": {},
   "source": [
    "### Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(prediction_df['Observed'],prediction_df['LR'])\n",
    "plt.xlabel('True Values [snow_depth]', fontsize=15)\n",
    "plt.ylabel('Predictions [snow_depth]', fontsize=15)\n",
    "plt.title(\"Linear Regression\")\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(prediction_df['Observed'],prediction_df['DNN'])\n",
    "plt.xlabel('True Values [snow_depth]', fontsize=15)\n",
    "plt.ylabel('Predictions [snow_depth]', fontsize=15)\n",
    "plt.title(\"Deep Neural Network\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-wings",
   "metadata": {},
   "source": [
    "### Visualize Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_error = prediction_df['Observed'] - prediction_df['LR']\n",
    "DNN_error = prediction_df['Observed'] - prediction_df['DNN']\n",
    "\n",
    "fa = plt.figure(figsize=(16,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "LR_error.hist()\n",
    "plt.xlabel('Error', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=15)\n",
    "plt.title(\"Linear Regression\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "DNN_error.hist()\n",
    "plt.xlabel('Error', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=15)\n",
    "plt.title(\"Deep Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-diana",
   "metadata": {},
   "source": [
    "### Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save('DNN')\n",
    "\n",
    "## To load model, use;\n",
    "model = tf.keras.models.load_model('DNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-plain",
   "metadata": {},
   "source": [
    "### Main Challenges of Machine Learning\n",
    "\n",
    "1. Bad Data (insufficient training data, nonrepresentative training data, irrelevant features)\n",
    "2. Bad Algorithm (overfitting the training the data, underfitting the training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-photographer",
   "metadata": {},
   "source": [
    "### Improving your Deep Learning Model\n",
    "\n",
    "Here are some ways to improve your deep neural network;\n",
    "\n",
    "1. Regularization (early stopping, dropout, etc)\n",
    "2. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-memorial",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Using same dataset, fit a deep neural network with four hidden layers. Set a random seed of 200 for reproducible results. The number of neuron in the hidden layers should be 512, 256, 128, and 64 respectively. Use the \"sigmoid\" activation for all hidden layers, optimize using the \"mean_absolute_error\", and set your number of epochs to 50. \n",
    "\n",
    "1. Plot the training and validation loss. COmpare to previous DNN model.\n",
    "2. Use new DNN model to predict snow depth with the test data. Evaluate performance using RMSE, $R^2$, and MAPE. Compare to results from previous DNN model.\n",
    "3. Plot predicted snow depth vs. the observed snow depth. Which DNN model do you think is most reliable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-setting",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. [Santiago on Twitter](https://twitter.com/svpino)\n",
    "2. [A Taxonomy of Big Data for Optimal Predictive Machine Learning and Data Mining by Ernest Fokoue](https://arxiv.org/abs/1501.00604)\n",
    "3. [An Introduction to Statistical Learning with Applications in R](https://link.springer.com/book/10.1007%2F978-1-4614-7138-7)\n",
    "4. [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 2nd Edition](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n",
    "5. [MIT 6.S191: Introduction to Deep Learning](https://www.youtube.com/channel/UCtslD4DGH6PKyG_1gFAX7sg)\n",
    "6. [Intro to Deep Learning (ML Tech Talks)](https://www.youtube.com/watch?v=AhE8RhPGH1A&t=2685s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-serve",
   "metadata": {},
   "source": [
    "## Appendix A\n",
    "\n",
    "### Linear Regression Weights\n",
    "\n",
    "Linear regression is a very simple supervised learning technique used for predicting a quantitative outcome variable. It is a parametric approach that lives up its name: it assumes a linear relationship between the outcome and the predictors. When only one predictor is involved, it is called $\\texttt{Simple Linear Regression}$ and when more than one predictor is involved, it is called $\\texttt{Multiple Linear Regression}$. For illustration purpose, the multiple linear regression will be used. \n",
    "\n",
    "Consider a dataset $\\mathcal{D}_n = \\left\\lbrace (\\textbf{x}_1, y_1), (\\textbf{x}_2, y_2), \\cdots, (\\textbf{x}_n, y_n) \\right\\rbrace$ where $\\textbf{x}_i^\\top \\equiv ({x}_{i1}, {x}_{i2}, \\cdots, {x}_{ik})$ denotes the $k$-dimensional vector of the features, and $y_i$ represents the corresponding outcome. In our case, $y_i$ is continuous. Mathematically, the predicted value ($\\hat{y}$) from the linear regression is given as;\n",
    "\n",
    "$$\\hat{y} = w_0 + \\textbf{w}^\\top \\textbf{x} = w_0 + w_1x_1 + w_2x_2 + \\cdots + w_kx_k$$\n",
    "\n",
    "where $\\textbf{w} = (w_1, w_2, \\cdots, w_k)^\\top \\in \\mathbb{R}^k$ is a vector of weights and  $w_0  \\in \\mathbb{R}$ is the bias/intercept term. The weights could be thought of as how each feature affects the prediction. If a feature receives a positive weight, then increasing the value of that feature\n",
    "increases the value of our prediction and vice versa.\n",
    "\n",
    "### Training a Linear Regression (Estimating the Optimal Weights)\n",
    "\n",
    "Just as mentioned above, there are different ways to quantify loss of a regression problem. In this tutorial, we shall limit our discussion to the squared error loss. For linear regression using the squared error loss, the loss function is written as\n",
    "\n",
    "$$\\mathcal{L} (y, \\hat{y}) = (y - \\hat{y})^2$$\n",
    "\n",
    "The corresponding empirical risk is given as\n",
    "\n",
    "\n",
    "$$\n",
    "\\widehat{\\mathcal{R}}_n(\\textbf{w}) = \\frac{1}{n^{(tr)}} \\sum_{i = 1}^{n^{(tr)}}(y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "In essence, we want to find a set of weights such that the empirical risk is minimized. The full empirical risk minimization for linear regression can be written as:\n",
    "\n",
    "$$\n",
    "\\textbf{w}^{\\star} = \\underset{\\textbf{w}}{{\\tt argmin}}\\  \\widehat{\\mathcal{R}}_n(\\textbf{w})\n",
    "$$\n",
    "\n",
    "The superscript ($tr$) indicates that all optimization is performed on the train set only.\n",
    "\n",
    "#### Optimizing the Empirical Risk\n",
    "\n",
    "The empirical risk as written above can be optimized in two ways;\n",
    "\n",
    "1. Using the Ordinary Least Squares approach\n",
    "2. Using Gradient Descent.\n",
    "\n",
    "**Ordinary Least Squares (OLS)**: OLS is an unconstrained optimization technique which minimizes the sum of squared residuals (squared error loss). The OLS approach provides a closed-form (formula) solution to the empirical risk minimization problem. In this tutorial, we shall state the closed form solution without proof. The closed form solution to the empirical risk minimization problem is written as;\n",
    "\n",
    "$$\n",
    "\\textbf{w}^{\\star} = (\\textbf{X}_{tr}^\\top \\textbf{X})^{-1} \\textbf{X}_{tr}^\\top \\textbf{y}_{tr}\n",
    "$$\n",
    "\n",
    "Where $\\textbf{X}_{tr}$ is the training data matrix and $\\textbf{y}_{tr}$ the vector of training dependent variable.\n",
    "\n",
    "**Gradient Decent**: the gradient descent algorithm is as follows:\n",
    "\n",
    "1. Initialize weights and biases with random numbers.\n",
    "2. Loop until convergence:\n",
    "    1. Compute the gradient; $\\frac{\\partial \\widehat{\\mathcal{R}}_n(\\textbf{w})}{\\partial \\textbf{w}}$\n",
    "    2. Updated weights; $\\textbf{w} \\leftarrow  \\textbf{w} - \\eta  \\frac{\\partial \\widehat{\\mathcal{R}}_n(\\textbf{w})}{\\partial \\textbf{w}}$\n",
    "3. Return the weights\n",
    "\n",
    "This is repeated for the bias and every single weight. The $\\eta$ in step 2(B) of the gradient descent algorithm is called the learning rate. It is a measure of how fast we descend the hill (since we are moving from large to small errors). Large learning rates may cause divergence and very small learning rates may take too many iterations before convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-powder",
   "metadata": {},
   "source": [
    "## Appendix B\n",
    "\n",
    "### Neural Network Weights\n",
    "\n",
    "Building up a NN with at least one hidden layer known as  multi layer perceptron (MLP) requires only repeating the  mathematical operation illustrated for the perceptron for every neuron in the hidden layer(s). Consider the feedforward NN in the above figure, each neuron in the hidden layer does the follow computation:\n",
    "\n",
    "$$z_j = w_{0,j}^{(1)} + \\textbf{w}_j^{\\top(1)}\\textbf{x}_i$$\n",
    "\n",
    "The final output (predicted value) is;\n",
    "\n",
    "$$\\hat{y}_i =h(w_{0,j}^{(2)} + \\textbf{w}_j^{\\top(2)}g(\\textbf{z}_i))$$\n",
    "\n",
    "where $\\textbf{w}_j^{(1)}$ is the vector of weights connecting the input layer to the $j$th neuron of the hidden layer, $\\textbf{w}_j^{(2)}$ is the vector of weights connecting the $j$th neuron in the hidden layer to the output neuron, and $h(\\cdot)$ is the activation function applied to the output layer (if any).\n",
    "\n",
    "### Training a Neural Network (Finding Optimal Weights for Prediction)\n",
    "\n",
    "A neural network using gradient descent approach as spelt out above. The only the difference is how the gradient ($\\frac{\\partial \\widehat{\\mathcal{R}}_n(\\textbf{w})}{\\partial \\textbf{w}}$) in step 2A. is performed. In neural networks, $\\frac{\\partial \\widehat{\\mathcal{R}}_n(\\textbf{w})}{\\partial \\textbf{w}}$ is performed using the Backpropagation approach. The details of backpropagation is beyond the scope of this tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
