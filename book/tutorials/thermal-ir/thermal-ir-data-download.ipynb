{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cooperative-question",
   "metadata": {},
   "source": [
    "# Downloading datasets for the thermal IR tutorial\n",
    "\n",
    "This is an optional notebook to demonstrate a method for accessing SnowEx data through the NASA EarthData API. \n",
    "\n",
    "Alternatively (as seen within the [thermal-ir-tutorial.ipynb](thermal-ir-tutorial.ipynb) notebook) you can access all data needed for the tutorial by running the following command in a terminal:\n",
    "\n",
    "`aws s3 sync --quiet s3://snowex-data/tutorial-data/thermal-ir/ /tmp/thermal-ir/`\n",
    "\n",
    ":::{admonition} Learning Objectives\n",
    "\n",
    "**At the conclusion of this tutorial, you will be able to:**\n",
    "- download datasets through the NASA Earthdata API\n",
    "- use data conversion tools to create geotiff files\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we'll need\n",
    "import pandas as pd\n",
    "from earthdata_api import earthdata_granule_search # for interfacing with the NASA Earthdata API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-figure",
   "metadata": {},
   "source": [
    "We are going to use a custom function (`earthdata_api.earthdata_granule_search()`) which can return a list of download URLs for files we search for. We can then use a utility like wget (with our EarthData credentials) to download the files from that list of URLs. \n",
    "\n",
    "Alternatively, we could search on the [EarthData](https://earthdata.nasa.gov/) or [NSIDC](https://nsidc.org/) website itself and download through the web interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-baker",
   "metadata": {},
   "source": [
    "**First, set up some of the search parameters we'll use for all the datasets we want to find:**\n",
    "* a bounding box for the Grand Mesa area, \n",
    "* and a date/time that we want to look for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-drawing",
   "metadata": {},
   "source": [
    "When defining a bounding box for the study area we want to look at, Earthdata will return anything that overlaps this spot.\n",
    "\n",
    "What we are specifying  below is a a tiny area, basicallcy a point search, but we can expand to a larger area if needed.\n",
    "\n",
    "Format the bounding box as a list of coordinates like `bounding_box = [ lower left longitude, lower left latitude, upper right longitude, upper right latitude ]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box = [-108.19, 39.03, -108.18, 39.04] # a little box on top of Grand Mesa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-kenya",
   "metadata": {},
   "source": [
    "We are going to look at one specific day, but we could search multiple dates/times. Those dates/times don't need to be a continuous time period, we could instead provide a list of dates/times. (That was my original motivation to write this function). Use either [pandas.Timestamp](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html), [datetime.datetime](https://docs.python.org/3/library/datetime.html#datetime.datetime), or [numpy.datetime64](https://numpy.org/doc/stable/reference/arrays.datetime.html) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestampsUTC = pd.Timestamp('2020-2-08') # February 8, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-guidance",
   "metadata": {},
   "source": [
    "---\n",
    "**Now we can find the first dataset we want, [ASTER](https://asterweb.jpl.nasa.gov/) satellite images.** Specificallfy we are going to download the AST_L1T product. Looking its [product documentation](https://lpdaac.usgs.gov/products/ast_l1tv003/) we see that it is available either as an HDF file with all bands, or as tif files with select visible or thermal bands.\n",
    "\n",
    "We want to download the hdf file, which we'll later convert to separate geotiffs for each band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information for the ASTER product we want\n",
    "product=\"AST_L1T\" # product name\n",
    "version=\"003\" # product version\n",
    "ext=\"hdf\" # geotiff files only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-mailing",
   "metadata": {},
   "source": [
    "To make sure we find the images from this day, we need to specify a window in time to search, which starts at our `timestampsUTC` time we already defined.\n",
    "\n",
    "To define this time window we specify two things, the size of the time window from our start time, and units of that size value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_time_window=1 # specify that we want to have a time window of 1\n",
    "search_time_window_units=\"d\" # specify time window units of Days, which together gives us 1 day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-boston",
   "metadata": {},
   "source": [
    "We also want to save the list of data access URLs so we can use wget to download them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_filepath=\"/tmp/thermal-ir/aster_download_list.txt\"\n",
    "# make sure this directory exists\n",
    "import os\n",
    "if not os.path.exists(\"/tmp/thermal-ir\"):\n",
    "    os.mkdir(\"/tmp/thermal-ir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-accused",
   "metadata": {},
   "source": [
    "**Finally, make the API call to get the list of files that match our search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = earthdata_granule_search(product, version, \n",
    "                                    bounding_box, timestampsUTC, \n",
    "                                    search_time_window, search_time_window_units, \n",
    "                                    list_filepath, ext)\n",
    "# we can print the list of files, but these are also saved to a file\n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-packet",
   "metadata": {},
   "source": [
    "**Now we can open a terminal and use wget to download the files that are listed.**\n",
    "\n",
    "Open a terminal and navigate to the data directory for this tutorial\n",
    "\n",
    "`cd /tmp/thermal-ir/`\n",
    "\n",
    "Use wget to download the files listed in aster_download_list.txt. This requires that you put your Earthdata username wherer it says YOUR_USERNAME. This will also prompt you for your Earthdata password.\n",
    "\n",
    "`wget --http-user=YOUR_USERNAME --ask-password --keep-session-cookies --auth-no-challenge=on -c -i aster_download_list.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-casino",
   "metadata": {},
   "source": [
    "**Convert hdf to geotiffs with ASTERL1T_hdf2tir.py script.** When we run this utility, the output data will be at-sensor radiance stored as Digital Numbers (DN) as uint16 data types.\n",
    "\n",
    "The ASTERL1T_hdf2tir.py is from:\n",
    "*NASA LP DAAC. (2020). Reformat and Georeference ASTER L1T HDF Files Data Prep Script. Ver. 1. NASA EOSDIS Land Processes Distributed Active Archive Center (LP DAAC), USGS/Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, USA. Accessed July 27, 2020. [https://git.earthdata.nasa.gov/projects/LPDUR/repos/aster-l1t/browse](https://git.earthdata.nasa.gov/projects/LPDUR/repos/aster-l1t/browse)*\n",
    "\n",
    "In the terminal, navigate to the directory containing the ASTERL1T_hdf2tir.py script\n",
    "\n",
    "`cd ~/website/book/tutorials/thermal-ir/ast-l1t/`\n",
    "\n",
    "Then run the script as follows to generate geotiff files for each ASTER band\n",
    "\n",
    "`python ASTERL1T_hdf2tif.py /tmp/thermal-ir/`\n",
    "\n",
    "You should now see the AST_L1T geotiff files in the data directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-religious",
   "metadata": {},
   "source": [
    "---\n",
    "**Download the second dataset we want, [ground-based temperature measurements](https://nsidc.org/data/SNEX20_VPTS_Raw/versions/1)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"SNEX20_VPTS_Raw\"\n",
    "version = \"1\"\n",
    "list_filepath=\"/tmp/thermal-ir/snow_temperature_download_list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = earthdata_granule_search(product, version, \n",
    "                                    bounding_box, timestampsUTC, \n",
    "                                    search_time_window, search_time_window_units, \n",
    "                                    list_filepath)\n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-logan",
   "metadata": {},
   "source": [
    "**Use wget to download these data**\n",
    "\n",
    "`wget --http-user=YOUR_USERNAME --ask-password --keep-session-cookies --auth-no-challenge=on -c -i snow_temperature_download_list.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-platform",
   "metadata": {},
   "source": [
    "Then unzip the folder we just downloaded (`-q` for \"quiet mode\" so it doesn't list every file as it unzips them, `-d` for the directory we want to unzip the files into)\n",
    "\n",
    "`unzip -q -d /tmp/thermal-ir/SNEX20_VPTS_Raw SNEX20_VPTS_Raw.zip`\n",
    "\n",
    "And you should see the folder \"Level-0\" with the raw ground-based snow temperature data inside now in your data directory\n",
    "\n",
    "The file we want is `/tmp/thermal-ir/SNEX20_VPTS_Raw/Level-0/snow-temperature-timeseries/CR10X_GM1_final_storage_1.dat`, this is a comma-delimited text file from the datalogger at snow pit 2S10\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-merchandise",
   "metadata": {},
   "source": [
    "```{warning} Note:\n",
    "The airborne TIR imagery from SnowEx 2020 is not yet publicly available through NSIDC, we will work with a sample image provided in this tutorial.\n",
    "```\n",
    "\n",
    "An alternate method for re-formatting the airborne IR mosaic NetCDF files is below. (You must first download the sample file with `aws s3 sync --quiet s3://snowex-data/tutorial-data/thermal-ir/ /tmp/thermal-ir/` in the terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --quiet s3://snowex-data/tutorial-data/thermal-ir/ /tmp/thermal-ir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-gregory",
   "metadata": {},
   "source": [
    "Open an airborne TIR mosaic NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open airborne TIR mosaic NetCDF file\n",
    "ds = xr.open_dataset('/tmp/thermal-ir/SNOWEX2020_IR_PLANE_2020Feb08_mosaicked_APLUW.nc',\n",
    "                     decode_times=False) # set decode_times to False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-portable",
   "metadata": {},
   "source": [
    "Inspect the dataset and its dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the dimensions in the dataset\n",
    "ds.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-handbook",
   "metadata": {},
   "source": [
    "There is an extra dimension (\"na\") that we will want to drop, we will want to rename some of the dims, assign coordinates to those dims, and add a coordinate reference system for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the extra \"na\" dimension from E_UTM, N_UTM, and time\n",
    "ds['E_UTM'] = ds['E_UTM'].isel(na=0, drop=True)\n",
    "ds['N_UTM'] = ds['N_UTM'].isel(na=0, drop=True)\n",
    "ds['time'] = ds['time'].isel(na=0, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-plate",
   "metadata": {},
   "source": [
    "Rename the dimensions to some easier to use names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename dims\n",
    "ds = ds.rename({\"pass\" : \"time\", \n",
    "                \"easting, x\" : \"easting\", \n",
    "                \"northing, y\" : \"northing\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-kazakhstan",
   "metadata": {},
   "source": [
    "This NetCDF file was generated in MATLAB, and the dates/times are in an epoch format. Use [utcfromtimestamp()](https://docs.python.org/3/library/datetime.html#datetime.datetime.utcfromtimestamp) and [isoformat()](https://docs.python.org/3/library/datetime.html#datetime.date.isoformat) to convert and reformat into a more convenient format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode matlab (epoch) format times\n",
    "utctime = [datetime.datetime.utcfromtimestamp(this_time).isoformat() for this_time in ds.time.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-rescue",
   "metadata": {},
   "source": [
    "Assign and then transpose coordinates in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign coordinates to the \"northing\",  \"easting\", and \"time\" dimensions\n",
    "ds = ds.assign_coords({\"time\": utctime, \"northing\": ds.N_UTM, \"easting\": ds.E_UTM})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose coords\n",
    "ds = ds.transpose(\"time\", \"northing\", \"easting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-paris",
   "metadata": {},
   "source": [
    "Set spatial dimensions then define which coordinate reference system the spatial dimensions are in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set spatial dimensions with rioxarray\n",
    "ds.rio.set_spatial_dims('easting', 'northing', inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the coordinate reference system for the spatial dims with rioxarray\n",
    "ds.rio.write_crs('epsg:32612', inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-keeping",
   "metadata": {},
   "source": [
    "Inspect the final, re-formatted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
